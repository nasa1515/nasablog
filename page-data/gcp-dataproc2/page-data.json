{"componentChunkName":"component---src-templates-blog-template-js","path":"/gcp-dataproc2/","result":{"data":{"cur":{"id":"c0b1f1d9-1ce1-51df-9998-f2fdcca0e5a7","html":"<p>머리말  </p>\n<p>저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다.<br>\n이번에는 DataProc Cluster에 Pyspark Script를 사용해서\n자동화 JOB을 만들어 보겠습니다. 파이썬을 첨가해서  </p>\n<hr>\n<h2 id=\"-data\" style=\"position:relative;\"><a href=\"#-data\" aria-label=\" data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✔ Data</h2>\n<p>Data의 경우에는 이전 포스트에서 다뤘었던 Covid-19의 기상 데이터를 기반으로 진행합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116961186-9e637480-acdd-11eb-906f-9e340165dee1.JPG\" alt=\"12312312\"></p>\n<ul>\n<li>용량 : 약 51GB</li>\n<li>행 : 542,304,210</li>\n</ul>\n<br/>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116961225-bfc46080-acdd-11eb-930e-ec68574417e5.JPG\" alt=\"2222\"></p>\n<ul>\n<li>데이터 형식 요약</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-python-script\" style=\"position:relative;\"><a href=\"#-python-script\" aria-label=\" python script permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👍 Python Script</h2>\n<p>위의 데이터에서 특정 그룹(나라, 날짜) 별로 MAX,MIN,AVG 값들의 평균 값을 구하는 스크립트 </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>context <span class=\"token keyword\">import</span> SparkContext\n<span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>session <span class=\"token keyword\">import</span> SparkSession\nsc <span class=\"token operator\">=</span> SparkContext<span class=\"token punctuation\">(</span><span class=\"token string\">'local'</span><span class=\"token punctuation\">)</span>\nspark <span class=\"token operator\">=</span> SparkSession<span class=\"token punctuation\">(</span>sc<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">type</span><span class=\"token punctuation\">(</span>spark<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n\nread_path <span class=\"token operator\">=</span> <span class=\"token string\">\"gs://nasa_us/\"</span>\nwrite_path <span class=\"token operator\">=</span> <span class=\"token string\">'gs://proc_result/result/'</span>\n\n\n<span class=\"token comment\"># def for columns cheange</span>\n\n<span class=\"token comment\"># ------------------------------------------------------------------</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">renameCols</span><span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">,</span> old_columns<span class=\"token punctuation\">,</span> new_columns<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> old_col<span class=\"token punctuation\">,</span>new_col <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>old_columns<span class=\"token punctuation\">,</span>new_columns<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        df <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>withColumnRenamed<span class=\"token punctuation\">(</span>old_col<span class=\"token punctuation\">,</span>new_col<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> df\n\n\n<span class=\"token comment\"># Old_columns</span>\nold_columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'avg(min_temperature_air_2m_f)'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'avg(max_temperature_air_2m_f)'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'avg(avg_temperature_air_2m_f)'</span>\n                <span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># New_columns</span>\nnew_columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'temperature_air_min_avg'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'temperature_air_max_avg'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'temperature_air_avg_avg'</span>\n                <span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># --------------------------------------------</span>\n<span class=\"token comment\"># ----------------------</span>\n\n<span class=\"token comment\"># Read CSV from GCS</span>\ndf <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>csv<span class=\"token punctuation\">(</span>read_path<span class=\"token punctuation\">,</span> header<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> inferSchema<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># data transform</span>\ndf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span><span class=\"token string\">'country'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'date'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">'min_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'max_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'avg_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\ndf2 <span class=\"token operator\">=</span> renameCols<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">,</span> old_columns<span class=\"token punctuation\">,</span> new_columns<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Write CSV to GCS</span>\ndf2<span class=\"token punctuation\">.</span>coalesce<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"header\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"true\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mode<span class=\"token punctuation\">(</span><span class=\"token string\">\"overwrite\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>csv<span class=\"token punctuation\">(</span>write_path<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>간단 설명 : GCS에서 CSV Format의 Data를 읽고 ETL 작업 후 결과를 GCS에 저장  </li>\n<li>Bigquery Table Data를 csv화 시키고 GCS에 저장하는 방법은 <a href=\"https://nasa1515.tech/gcp_dataproc/\">이전포스트</a>를 확인하세요</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-dataproc-job-생성\" style=\"position:relative;\"><a href=\"#-dataproc-job-%EC%83%9D%EC%84%B1\" aria-label=\" dataproc job 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👌 DataProc Job 생성</h2>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962299-91945000-ace0-11eb-8e8f-20ea0f9f5b15.JPG\" alt=\"333\"></p>\n<ul>\n<li>위와 같이 DataProc - JOB -> 작업 제출로 JOB을 생성합니다.  </li>\n</ul>\n<br/>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962386-c6a0a280-ace0-11eb-96f5-aaaad00c4588.JPG\" alt=\"44444\"></p>\n<ul>\n<li>Cluster는 실행 할 Cluster를 지정합니다.</li>\n<li>작업 유형은 Pyspark를 선택합니다. </li>\n<li>Python File의 경우 미리 GCS에 올려놓고 지정하면 됩니다.  </li>\n</ul>\n<br/>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962498-12ebe280-ace1-11eb-835a-2b85ed26c91c.JPG\" alt=\"캡처55555\"></p>\n<ul>\n<li>위와 같이 해당 작업이 생성되면서 실행되게 되고<br>\nJOB의 완료 된 후에는 결과 및 로그가 출력되게 됩니다.  </li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-결과-확인\" style=\"position:relative;\"><a href=\"#-%EA%B2%B0%EA%B3%BC-%ED%99%95%EC%9D%B8\" aria-label=\" 결과 확인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🐱‍🏍 결과 확인</h2>\n<p>Script 실행대로 GCS에 ETL 결과 파일이 다음과 같이 저장되었습니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962590-5c3c3200-ace1-11eb-8614-8e54e3664677.JPG\" alt=\"66666666\"></p>\n<br/>\n<p>그럼 해당 CSV 파일을 기반으로 BigQuery에 Table을 만들어 보겠습니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962680-a0c7cd80-ace1-11eb-84fa-c3e8ca5a092b.JPG\" alt=\"65446565464\"></p>\n<br/>\n<p>데이터를 확인해보면 Script에서 실행 된 ETL 결과만 남아있는 것을 확인 가능합니다.</p>\n<ul>\n<li>\n<p>스키마 데이터</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116963266-59424100-ace3-11eb-9d2a-e3549f04bcae.JPG\" alt=\"77777\"></p>\n<br/>\n</li>\n<li>\n<p>결과 데이터</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116963305-76770f80-ace3-11eb-800d-7cb6f0762ef5.JPG\" alt=\"캡처332131\"></p>\n<br/>\n</li>\n</ul>\n<h2 id=\"끝\" style=\"position:relative;\"><a href=\"#%EB%81%9D\" aria-label=\"끝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>끝!</h2>\n<hr>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며…</h2>\n<p>DataProc에 대해서는 어떻게 사용하는지 대충 알아 본 것 같습니다.<br>\n그래서 다음 포스트에서는 Proc과 동일한 서비스를 제공하는 DataBricks를 사용해보겠습니다.  </p>\n<hr>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#-data\">✔ Data</a></li>\n<li><a href=\"#-python-script\">👍 Python Script</a></li>\n<li><a href=\"#-dataproc-job-%EC%83%9D%EC%84%B1\">👌 DataProc Job 생성</a></li>\n<li><a href=\"#-%EA%B2%B0%EA%B3%BC-%ED%99%95%EC%9D%B8\">🐱‍🏍 결과 확인</a></li>\n<li><a href=\"#%EB%81%9D\">끝!</a></li>\n<li><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며…</a></li>\n</ul>\n</div>","excerpt":"머리말   저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다. 이번에는 DataProc Cluster에 Pyspark Script를 사용해서 \n자동화 JOB을 만들어 보겠습니다. 파이썬을 첨가해서   ✔ Data Data의 경우에는 이전 포스트에서 다뤘었던 Covid-19의 기상 데이터를 기반으로 진행합니다.   12312312 용량 : 약 51GB 행 : 542,304,210 2222 데이터 형식 요약 👍 Python Script 위의 데이터에서 특정 그룹(나라, 날짜) 별로 MAX,MIN,AVG 값들의 평균 값을 구하는 스크립트  간단 설명 : GCS에서 CSV Format의 Data를 …","frontmatter":{"date":"September 10, 2021","title":"[DATA, GCP] - GCP DataProc 2탄 Pyspark JOB Access","categories":"GCP DATA","author":"nasa1515","emoji":"🤦‍♂️"},"fields":{"slug":"/gcp-dataproc2/"}},"next":{"id":"f1eefdc3-7c74-544e-b4c9-728dcf1c72c7","html":"<p>머리말  </p>\n<p>이번에는 DataProc(Hadoop/Spark)를 사용하여\n대용량의 데이터를 처리하는 방법에 대해서 다룹니다. 물론 파이썬을 첨가해서  </p>\n<hr>\n<h2 id=\"-dataproc에-대해서\" style=\"position:relative;\"><a href=\"#-dataproc%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C\" aria-label=\" dataproc에 대해서 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✔ DataProc에 대해서..</h2>\n<p><em>Dataproc은 일괄 처리, 쿼리, 스트리밍, 머신 러닝에 오픈소스 데이터 도구를 활용할 수 있는 관리형 Spark 및 Hadoop 서비스입니다.</em><br>\n즉 지금까지 귀찮게 Spark, Hadoop을 연동하는 과정을 없애고 사용만하면 되는 서비스라고 볼 수 있습니다.  </p>\n<p>여기서 DataFlow와 DataProc의 차이에 대해서 궁금증이 생겼는데<br>\n두 툴 모두 ETL을 하는 툴에 대해서는 공통점을 가지고 있지만<br>\nDataFlow는 Serverless 서비스로 Streaming, Batch Flow를 Code로 관리하고 싶으면 사용하고<br>\nDataProc은 기존에 HDFS 같은 Hadoop EcoSystem에 종속되어 있는 시스템을 가지고 있다면 사용하기 좋다고 합니다.</p>\n<br/>\n<hr>\n<h2 id=\"-dataproc-cluster-생성\" style=\"position:relative;\"><a href=\"#-dataproc-cluster-%EC%83%9D%EC%84%B1\" aria-label=\" dataproc cluster 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✌ DataProc Cluster 생성</h2>\n<br/>\n<ul>\n<li>\n<p>GCP 탐색 메뉴 > Dataproc > 클러스터 선택 > 클러스터 만들기</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116354676-b8134080-a833-11eb-8b5a-249126ff2798.png\" alt=\"다운로드\"></p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>클러스터 필드 설정 (이름을 제외한 나머지 부분은 기본값)</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116355090-4f789380-a834-11eb-8880-311c70b982b2.JPG\" alt=\"캡처\"></p>\n<br/>\n</li>\n<li>\n<p>프로비저닝 과정을 3분정도 거치고 다음과 같이 생성이 완료됩니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116356531-5b655500-a836-11eb-873e-f5701fbc4d9a.JPG\" alt=\"2\"></p>\n</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-data-준비하기\" style=\"position:relative;\"><a href=\"#-data-%EC%A4%80%EB%B9%84%ED%95%98%EA%B8%B0\" aria-label=\" data 준비하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🙌 Data 준비하기</h2>\n<p>이번 포스트에서는 BigQuery에서 공개적으로 제공하는 DataSet을 이용합니다.  </p>\n<p>Dataproc Cluster는 GCS Connector를 기본으로 제공하여<br>\n다른 설정없이 GCS에 있는 데이터에 바로 액세스가 가능합니다.<br>\n저는 이를 이용해서 BigQuery의 공개 DataSet의 특정 테이블을 Cloud Storage로 Export하여<br>\nExport한 데이터에 바로 접근하여 사용, 퍼포먼스 테스트를 해보겠습니다.  </p>\n<br/>\n<ul>\n<li>\n<p>데이터 정보  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116357157-24437380-a837-11eb-9047-048e8e5a018d.JPG\" alt=\"캡처3\"></p>\n<ul>\n<li>Table ID : bigquery-public-data:covid19<em>weathersource</em>com.postal<em>code</em>day_history</li>\n<li>Table 크기 : 약 300 </li>\n</ul>\n<br/>\n</li>\n<li>\n<p>데이터 형식 </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116357277-476e2300-a837-11eb-920c-682f3e9dfa19.JPG\" alt=\"캡처4\"></p>\n<p>데이터의 내용은 나라 별 COVID-19의 기상상태 데이터입니다.  </p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>Cloude Storage 생성 (GCS) - 쿼리 결과 데이러(CSV)틑 쌓는 곳  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116358291-720cab80-a838-11eb-8d31-a13169891652.JPG\" alt=\"캡처5\"></p>\n<ul>\n<li>Region을 Bigquery와 맞춰주어야 합니다. </li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>BigQuery DataSet, Table 생성 (쿼리 결과 데이터를 쌓는 곳)</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116358650-d7609c80-a838-11eb-8791-4acddef9cb46.JPG\" alt=\"캡처6\"></p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>저는 Python으로 간단한 코드를 작성해서 다음과 같이 데이터를 분류했습니다.  </p>\n<p>공개 DataSet에서 쿼리 결과를 다른 테이블에 저장하는 코드 </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> google<span class=\"token punctuation\">.</span>cloud <span class=\"token keyword\">import</span> bigquery\n\n<span class=\"token comment\"># Construct a BigQuery client object.</span>\nclient <span class=\"token operator\">=</span> bigquery<span class=\"token punctuation\">.</span>Client<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># TODO(developer): Set table_id to the ID of the destination table.</span>\ntable_id <span class=\"token operator\">=</span> <span class=\"token string\">\"lws-cloocus.ustest.ustable\"</span>\n\njob_config <span class=\"token operator\">=</span> bigquery<span class=\"token punctuation\">.</span>QueryJobConfig<span class=\"token punctuation\">(</span>destination<span class=\"token operator\">=</span>table_id<span class=\"token punctuation\">)</span>\n\nsql <span class=\"token operator\">=</span> <span class=\"token string\">'SELECT * FROM `bigquery-public-data.covid19_weathersource_com.postal_code_day_history` LIMIT 34230421'</span>\n\n<span class=\"token comment\"># Start the query, passing in the extra configuration.</span>\nquery_job <span class=\"token operator\">=</span> client<span class=\"token punctuation\">.</span>query<span class=\"token punctuation\">(</span>sql<span class=\"token punctuation\">,</span> job_config<span class=\"token operator\">=</span>job_config<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Make an API request.</span>\nquery_job<span class=\"token punctuation\">.</span>result<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Wait for the job to complete.</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Query results loaded to the table {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>table_id<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>아셔야 하는 건 Table을 복사하는 것과 데이터만(쿼리결과)복사하는 것은 다릅니다.<br>\nTable을 그대로 복사하게되면 Table의 정보까지 저장됩니다..</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>해당 Code를 실행시키게 되면 다음과 같이 특정 Table에 쿼리결과가 저장됩니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116361301-c9f8e180-a83b-11eb-90fb-a61cfbbd8fc9.JPG\" alt=\"캡처\"></p>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<p>쿼리결과가 저장되어있는 Table의 데이터를 csv로 변환해서 GCS로 저장  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Source option</span>\nproject <span class=\"token operator\">=</span> <span class=\"token string\">\"lws-cloocus\"</span>\ndataset_id <span class=\"token operator\">=</span> <span class=\"token string\">\"ustest\"</span>\ntable_id <span class=\"token operator\">=</span> <span class=\"token string\">\"ustable\"</span>\n\n\n<span class=\"token comment\"># 용량 많은 Table (1G이상)은 * 정규표현식으로 Table 읽어서 csv화 시켜야 함.</span>\ndestination_uri <span class=\"token operator\">=</span> <span class=\"token string\">\"gs://{}/{}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>bucket_name<span class=\"token punctuation\">,</span> <span class=\"token string\">\"result*.csv\"</span><span class=\"token punctuation\">)</span>\ndataset_ref <span class=\"token operator\">=</span> bigquery<span class=\"token punctuation\">.</span>DatasetReference<span class=\"token punctuation\">(</span>project<span class=\"token punctuation\">,</span> dataset_id<span class=\"token punctuation\">)</span>\ntable_ref <span class=\"token operator\">=</span> dataset_ref<span class=\"token punctuation\">.</span>table<span class=\"token punctuation\">(</span>table_id<span class=\"token punctuation\">)</span>\n\nextract_job <span class=\"token operator\">=</span> client<span class=\"token punctuation\">.</span>extract_table<span class=\"token punctuation\">(</span>\n    table_ref<span class=\"token punctuation\">,</span>\n    destination_uri<span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># Location must match that of the source table.</span>\n    location<span class=\"token operator\">=</span><span class=\"token string\">\"US\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># API request</span>\nextract_job<span class=\"token punctuation\">.</span>result<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Waits for job to complete.</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>\n    <span class=\"token string\">\"Exported {}:{}.{} to {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>project<span class=\"token punctuation\">,</span> dataset_id<span class=\"token punctuation\">,</span> table_id<span class=\"token punctuation\">,</span> destination_uri<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>BigQeury에서 Data를 export 할 때 한번에 1GB 단위까지 밖에 지원되지 않습니다.<br>\n때문에 * 와일드카드를 사용해서 CSV File을 분리해줘야 합니다.  </li>\n</ul>\n<br/>\n</li>\n<li>\n<p>해당 코드를 실행시키면 다음과 같이 GCS에 Data가 저장됩니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116361977-8783d480-a83c-11eb-8d1e-5d447ce71323.JPG\" alt=\"캡처3\"></p>\n<ul>\n<li>다음과 같이 용량이 일정하게 나눠서 저장됩니다.  </li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>저장된 CSV File을 Local로 다운받아서 NotePad로 확인해보죠  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116362297-ddf11300-a83c-11eb-8944-4694fb94bd4e.JPG\" alt=\"23\"></p>\n<ul>\n<li>다음과 같이 맨 윗줄은 헤더 정보, 나머지는 데이터 값만 저장됩니다.  </li>\n</ul>\n</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-jupyter-notebook-연결\" style=\"position:relative;\"><a href=\"#-jupyter-notebook-%EC%97%B0%EA%B2%B0\" aria-label=\" jupyter notebook 연결 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👍 Jupyter Notebook 연결</h2>\n<p>이제 간단한 쿼리 테스트를 진행하기 위해 Jupyter Notebook을 연결 하려고 했는데…<br>\n보니깐 Cluster를 잘못 생성했네요 아래 구성요소 GW를 사용하는 옵션을 체크해야합니다.  </p>\n<ul>\n<li>\n<p>구성요소 게이트웨이 </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116366348-2a3e5200-a841-11eb-81c1-c83a9f629154.JPG\" alt=\"캡처\"></p>\n<ul>\n<li>게이트웨이 옵션 체크</li>\n<li>구성요소에서 Jupyter Notebook 체크 </li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>클러스터가 새롭게 만들어졌다면 클러스터 정보-> 웹 인터페이스로 접속합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116367440-527a8080-a842-11eb-9843-2fdb4cfdecfa.JPG\" alt=\"캡처3\"></p>\n<ul>\n<li>그럼 다음과 같이 JupyterLab GW link가 생기고 접속합니다.  </li>\n</ul>\n<br/>\n</li>\n<li>\n<p>그럼 다음과 같이 DataProc Cluster와 연결된 Jupyter Page에 접속이 가능합니다. </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116367646-85bd0f80-a842-11eb-8005-406031f0ca44.JPG\" alt=\"캡처4\"></p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>이 후에 GCS에 저장된 csv를 읽는 것도 가능합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116504392-21a65400-a8f3-11eb-9e7e-8295718c17e2.JPG\" alt=\"33333\"></p>\n<br/>\n</li>\n<li>\n<p>저는 쿼리 결과 시간이나, table 형태로 결과를 보고 싶어서 extension을 추가 설치 했습니다. </p>\n<br/>\n<ul>\n<li>\n<p>Jupyter에서 Terminal 창을 연 뒤 아래 명령어로 설치합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"cs\"><pre class=\"language-cs\"><code class=\"language-cs\"><span class=\"token preprocessor property\"># pip install jupyterlab_execute_time</span></code></pre></div>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>설치가 완료 된 뒤 연결된 WEB을 새로고침 하면 execure-time 이 설치되어 있습니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116510416-d5154580-a8ff-11eb-86ff-8ea55e585217.JPG\" alt=\"캡처444\"></p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>그 후 Settings - Advanced Settings editor - Notebook에 아래 코드를 추가합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116514663-4b1cab00-a906-11eb-9071-34c3d15fc616.JPG\" alt=\"22222222\"></p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>그럼 다음과 같이 쿼리 실행 시간이 출력됩니다.!</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116514759-6be50080-a906-11eb-9860-534243a6388f.JPG\" alt=\"3213121\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>저는 필요해보이는 추가 Extention을 더 설치해줬습니다.  </p>\n<br/>\n<ul>\n<li>variableinspector</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"cs\"><pre class=\"language-cs\"><code class=\"language-cs\"><span class=\"token preprocessor property\"># pip install lckr-jupyterlab-variableinspector</span></code></pre></div>\n<br/>\n<ul>\n<li>TOC</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"cs\"><pre class=\"language-cs\"><code class=\"language-cs\"><span class=\"token preprocessor property\"># jupyter nbextension enable toc2/main</span></code></pre></div>\n</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-pyspark-test\" style=\"position:relative;\"><a href=\"#-pyspark-test\" aria-label=\" pyspark test permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👏 Pyspark Test</h2>\n<p>위에서도 Test를 해봤지만 그래도 몇가지 pyspark 함수를 사용해보겠습니다.  </p>\n<br/>\n<ul>\n<li>\n<p>CSV File 읽어오기 (GCS에 있는)  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># df = spark.read.csv(\"gs://nasa_us/\", header=True, inferSchema=True)</span></code></pre></div>\n<ul>\n<li>gs://nasa_us/ 경로에 있는 모든 파일을 읽습니다.  </li>\n<li>read.csv 옵션인 header, inferSchema를 사용했습니다.  </li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116836339-17d96500-ac01-11eb-9ab5-707e5ae65e2c.JPG\" alt=\"2221313\"></p>\n<ul>\n<li>약 45.96s 가 소요 되었습니다.  </li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>해당 DataFrame의 Row 반환 </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># df.show(10)    -- Row 10개 반환</span>\n<span class=\"token comment\"># df.count()     -- Row 갯수 반환</span></code></pre></div>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116836737-ac909280-ac02-11eb-8dcc-dac053327581.JPG\" alt=\"11111\"></p>\n<ul>\n<li>show에는 546ms 가 소요 되었습니다.</li>\n<li>count에는 13.81s 가 소요 되었습니다.  </li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>해당 DataFrame의 Summary 값 반환</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># df.summary().show()</span></code></pre></div>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116841196-68f25480-ac13-11eb-872b-1fc2019bdc29.JPG\" alt=\"222222\"></p>\n<ul>\n<li>Summary에는 4m 16.06s 가 소요 되었습니다.</li>\n</ul>\n</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며…</h2>\n<p>DataProc을 사용해보면서 Spark에 대해서도 다시 알아 갈 생각입니다.<br>\n그래서 다음 포스트에서는 Pyspark의 문법에 대해서 포스팅 예정입니다.  </p>\n<hr>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#-dataproc%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C\">✔ DataProc에 대해서..</a></li>\n<li><a href=\"#-dataproc-cluster-%EC%83%9D%EC%84%B1\">✌ DataProc Cluster 생성</a></li>\n<li><a href=\"#-data-%EC%A4%80%EB%B9%84%ED%95%98%EA%B8%B0\">🙌 Data 준비하기</a></li>\n<li><a href=\"#-jupyter-notebook-%EC%97%B0%EA%B2%B0\">👍 Jupyter Notebook 연결</a></li>\n<li><a href=\"#-pyspark-test\">👏 Pyspark Test</a></li>\n<li><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며…</a></li>\n</ul>\n</div>","frontmatter":{"date":"September 08, 2021","title":"[DATA, GCP] - GCP DataProc spark Cluster로 ETL 후 BigQuery에 적재","categories":"GCP DATA","author":"nasa1515","emoji":"🤦‍♂️"},"fields":{"slug":"/gcp-dataproc/"}},"prev":{"id":"14668db0-2f50-511a-abf9-9d92fcdc88bd","html":"<br/>\n<p>머리말  </p>\n<p>저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다.<br>\n이번에는 GCP에서 파트너 SaaS형태로 제공해주는 DataBricks를 사용해서<br>\n지난번과 동일한 데이터, 스트립트를 이용해서 성능이나, 사용법에 대한 테스트를 해봤습니다.<br>\n물논 이번에도 파이썬을 첨가해서  </p>\n<hr>\n<h2 id=\"-databricks\" style=\"position:relative;\"><a href=\"#-databricks\" aria-label=\" databricks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✔ DataBricks?</h2>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117228331-cf65b580-ae53-11eb-9b9d-81bd0a524677.png\" alt=\"og-databricks\"></p>\n<p>Databricks란?<br>\n간단 요약해서 Spark,Hadoop 등 빅데이터 관련 솔루션 실행환경을 제공하는 클라우드 서비스입니다.<br>\n통합 분석 플랫폼으로, 한 WorkSpace내에서 여러 서비스를 사용해 모든 분석이 가능합니다.<br>\n이전에 Spark, Hadoop을 ON-Premise 환경에 설치한 포스트를 확인해보시면 알겠지만<br>\nJDK부터 연동해야 하는 부분이 매우 귀찮고 오랜 시간이 걸리게 됩니다.<br>\nDataBricks를 사용하면 설치, 설정 부분 없이 바로 사용이 가능하다는 장점이 있습니다.  </p>\n<p>DataBricks는 아래 작업들을 한 WorkSpace에서 지원합니다.  </p>\n<ul>\n<li>reports</li>\n<li>dashboards</li>\n<li>ETL 작업 실행 (Extract, Transform, Load)</li>\n<li>머신러닝, 스트림 작업</li>\n<li>아파치 Spark보다 더 optimized.</li>\n<li>Databricks 서버와 실시간으로 interaction</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-gcp-databricks\" style=\"position:relative;\"><a href=\"#-gcp-databricks\" aria-label=\" gcp databricks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👌 GCP DataBricks?</h2>\n<p>GCP, Azure, AWS 등 3사 Public Cloud는 이미 DataBricks를 SaaS, PaaS 형태로 지원하고 있습니다.<br>\nGCP의 경우 아직 도입된지 1년이 채 안되서 불안정한 부분도 있고 Korea Region도 지원하지 않습니다.<br>\n아직 GA일정도 나오지 않은 상태구요…(AWS,AZURE는 다 있는데…)<br>\n그래서 이번 포스트에서는 어쩔 수 없이 US Region에서의 테스트를 진행하겠습니다.  </p>\n<br/>\n<ul>\n<li>\n<h4 id=\"databricks-사용하기구독\" style=\"position:relative;\"><a href=\"#databricks-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0%EA%B5%AC%EB%8F%85\" aria-label=\"databricks 사용하기구독 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataBricks 사용하기(구독)</h4>\n<p>GCP에서 DataBricks를 사용하기 위해서는 다음과 같이 구독을 먼저 진행해야 합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117228743-a0037880-ae54-11eb-9362-11dd61314007.JPG\" alt=\"da\"></p>\n<ul>\n<li>구독 이후에 DataBricks의 구매 스펙을 정하고 DataBricks Dashborad로 이동하면 됩니다.  </li>\n</ul>\n<br/>\n</li>\n<li>\n<h4 id=\"workspace-생성\" style=\"position:relative;\"><a href=\"#workspace-%EC%83%9D%EC%84%B1\" aria-label=\"workspace 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>WorkSpace 생성</h4>\n<p>Dashborad에서 아래와 같이 사용 할 WorkSpace를 생성하면 됩니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117228897-025c7900-ae55-11eb-941b-597f74ec3f45.JPG\" alt=\"캡처2\"></p>\n<br/>\n</li>\n<li>\n<h4 id=\"workspace-접속\" style=\"position:relative;\"><a href=\"#workspace-%EC%A0%91%EC%86%8D\" aria-label=\"workspace 접속 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>WorkSpace 접속</h4>\n<p>WorkSpace를 생성 후 URL에 접속하면 드디어 데이터 작업을 할 수 있습니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117229354-d42b6900-ae55-11eb-839c-bc1f7979ed4b.JPG\" alt=\"123123123\"></p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<h4 id=\"databricks-cluster-생성\" style=\"position:relative;\"><a href=\"#databricks-cluster-%EC%83%9D%EC%84%B1\" aria-label=\"databricks cluster 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataBricks Cluster 생성</h4>\n<p>WorkSpace의 Cluster Tab에서 Create Cluster를 클릭해 생성합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117230334-d68ec280-ae57-11eb-939d-295ed700acea.JPG\" alt=\"33333333333333\"></p>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<h4 id=\"저는-다음과-같은-spec으로-cluster를-생성했습니다\" style=\"position:relative;\"><a href=\"#%EC%A0%80%EB%8A%94-%EB%8B%A4%EC%9D%8C%EA%B3%BC-%EA%B0%99%EC%9D%80-spec%EC%9C%BC%EB%A1%9C-cluster%EB%A5%BC-%EC%83%9D%EC%84%B1%ED%96%88%EC%8A%B5%EB%8B%88%EB%8B%A4\" aria-label=\"저는 다음과 같은 spec으로 cluster를 생성했습니다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>저는 다음과 같은 Spec으로 Cluster를 생성했습니다.</h4>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117380571-f20bd300-af14-11eb-9cae-69720f7c2043.JPG\" alt=\"2222\"></p>\n<ul>\n<li>Nmae : Cluster01 </li>\n<li>Runtime Version : 8.1  </li>\n<li>Worker Type : n2-standard-8 </li>\n<li>Advanced Option Tab을 열어 Google Service Account 입력\n주의 : Service Account는 GCS에 권한이 있어야 합니다.</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>이제 Notebook을 생성하고 GCS를 DBFS에 Mount해서 사용하시면 됩니다.  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">bucket_name <span class=\"token operator\">=</span> <span class=\"token string\">\"nasagcp\"</span>\nmount_name <span class=\"token operator\">=</span> <span class=\"token string\">\"gcpdata\"</span>\ndbutils<span class=\"token punctuation\">.</span>fs<span class=\"token punctuation\">.</span>mount<span class=\"token punctuation\">(</span><span class=\"token string\">\"gs://%s\"</span> <span class=\"token operator\">%</span> bucket_name<span class=\"token punctuation\">,</span> <span class=\"token string\">\"/mnt/%s\"</span> <span class=\"token operator\">%</span> mount_name<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>다음과 같은 형식으로 사용하시면 됩니다.</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>저는 이전에 짜놨었던 스크립트를 다음과 같은 형식으로 사용했습니다. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">bucket_name <span class=\"token operator\">=</span> <span class=\"token string\">\"nasagcp\"</span>\nmount_name <span class=\"token operator\">=</span> <span class=\"token string\">\"gcpdat11\"</span>\ndbutils<span class=\"token punctuation\">.</span>fs<span class=\"token punctuation\">.</span>mount<span class=\"token punctuation\">(</span><span class=\"token string\">\"gs://%s\"</span> <span class=\"token operator\">%</span> bucket_name<span class=\"token punctuation\">,</span> <span class=\"token string\">\"/mnt/%s\"</span> <span class=\"token operator\">%</span> mount_name<span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>context <span class=\"token keyword\">import</span> SparkContext\n<span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>session <span class=\"token keyword\">import</span> SparkSession\n\n\n<span class=\"token comment\"># ------------------------------------------------------------------</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">renameCols</span><span class=\"token punctuation\">(</span>df1<span class=\"token punctuation\">,</span> old_columns<span class=\"token punctuation\">,</span> new_columns<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> old_col<span class=\"token punctuation\">,</span>new_col <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>old_columns<span class=\"token punctuation\">,</span>new_columns<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        d1f <span class=\"token operator\">=</span> df1<span class=\"token punctuation\">.</span>withColumnRenamed<span class=\"token punctuation\">(</span>old_col<span class=\"token punctuation\">,</span>new_col<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> df1\n\n\n<span class=\"token comment\"># Old_columns</span>\nold_columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'avg(min_temperature_air_2m_f)'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'avg(max_temperature_air_2m_f)'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'avg(avg_temperature_air_2m_f)'</span>\n                <span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># New_columns</span>\nnew_columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'temperature_air_min_avg'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'temperature_air_max_avg'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'temperature_air_avg_avg'</span>\n                <span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># --------------------------------------------</span>\n<span class=\"token comment\"># ----------------------</span>\n\n<span class=\"token comment\"># Read CSV from GCS</span>\ndf_lee <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>csv<span class=\"token punctuation\">(</span><span class=\"token string\">\"/mnt/gcpdat11/\"</span><span class=\"token punctuation\">,</span> header<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> inferSchema<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># data transform</span>\ndf_lee <span class=\"token operator\">=</span> df_lee<span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span><span class=\"token string\">'country'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'date'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">'min_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'max_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'avg_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span>desc<span class=\"token punctuation\">(</span><span class=\"token string\">'country'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>orderBy<span class=\"token punctuation\">(</span><span class=\"token string\">'date'</span><span class=\"token punctuation\">)</span>\n\ndf_result <span class=\"token operator\">=</span> renameCols<span class=\"token punctuation\">(</span>df_lee<span class=\"token punctuation\">,</span> old_columns<span class=\"token punctuation\">,</span> new_columns<span class=\"token punctuation\">)</span>\n\ncountry1 <span class=\"token operator\">=</span> df_result<span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span><span class=\"token string\">\"country\"</span><span class=\"token punctuation\">)</span>\ncountry_dis10 <span class=\"token operator\">=</span> df_result<span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span><span class=\"token string\">\"country\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>distinct<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"country_count =\"</span><span class=\"token punctuation\">,</span>country_dis10<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># Write CSV to GCS</span>\ndf_result<span class=\"token punctuation\">.</span>coalesce<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"header\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"true\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mode<span class=\"token punctuation\">(</span><span class=\"token string\">\"overwrite\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>csv<span class=\"token punctuation\">(</span><span class=\"token string\">\"/mnt/gcpdat11/dbfsre/\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>이전에 받아놨던 Covid-19 기상 데이터를 정렬하는 Code 입니다.  </li>\n</ul>\n</li>\n</ul>\n<br/> \n<hr>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며…</h2>\n<p>사실 DataBricks는 사용법에 대한 가이드를 남기기에는 너무 간편합니다..<br>\n그래서 그나마 어려움이 있을 것 같은 DBFS Mount 부분만 설명했습니다.  </p>\n<hr>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#-databricks\">✔ DataBricks?</a></li>\n<li><a href=\"#-gcp-databricks\">👌 GCP DataBricks?</a></li>\n<li><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며…</a></li>\n</ul>\n</div>","frontmatter":{"date":"September 12, 2021","title":"[DATA, GCP] - GCP DataBricks 사용기","categories":"GCP DATA","author":"nasa1515","emoji":"🤦‍♂️"},"fields":{"slug":"/data-databricks/"}},"site":{"siteMetadata":{"siteUrl":"https://nasa1515.com","comments":{"utterances":{"repo":"nasa1515/nasablog"}}}}},"pageContext":{"slug":"/gcp-dataproc2/","nextSlug":"/gcp-dataproc/","prevSlug":"/data-databricks/"}},"staticQueryHashes":["1073350324","2938748437"]}