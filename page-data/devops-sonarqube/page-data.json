{"componentChunkName":"component---src-templates-blog-template-js","path":"/devops-sonarqube/","result":{"data":{"cur":{"id":"0c0f7578-bb81-557a-8d80-32a4858eeb47","html":"<p>머리말  </p>\n<p>이번 포스트로 이제 파이프라인에서 동작하는 전체적인 보안툴에 대한 포스트는 끝났습니다.<br>\n최종적으로는   </p>\n<ol>\n<li>SonarQube로 Build 될 이미지의 소스코드에 대한 전략적 정적분석을   </li>\n<li>Anchore로 빌드된 이미지에 대한 분석을  </li>\n<li>OWASP ZAP으로 배포 된 서비스에 대한 동적분석  </li>\n</ol>\n<p>위 세가지 보안 항복을 Jenkins를 이용해 자동화 하였습니다.</p>\n<hr>\n<ul>\n<li>\n<p>사용 할 툴을 다음과 같습니다.  </p>\n<ul>\n<li>Jenkins</li>\n<li>Sonarqube</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"-sonarqube-\" style=\"position:relative;\"><a href=\"#-sonarqube-\" aria-label=\" sonarqube  permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✔ SonarQube ??</h2>\n<p>위키백과 왈</p>\n<p><em>소나큐브(SonarQube, 이전 이름: 소나/Sonar)는 20개 이상의 프로그래밍 언어에서 버그, 코드 스멜, 보안 취약점을 발견할 목적으로 정적 코드 분석으로 자동 리뷰를 수행하기 위한 지속적인 코드 품질 검사용 오픈 소스 플랫폼이다.<br>\n소나큐브는 중복 코드, 코딩 표준, 유닛 테스트, 코드 커버리지, 코드 복잡도, 주석, 버그 및 보안 취약점의 보고서를 제공한다.</em></p>\n<p>음 읽어보니 개발자들에게 유용한 정적분석 툴입니다.</p>\n<br/>\n<hr>\n<h3 id=\"sonarqube-설치\" style=\"position:relative;\"><a href=\"#sonarqube-%EC%84%A4%EC%B9%98\" aria-label=\"sonarqube 설치 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SonarQube 설치</h3>\n<ul>\n<li>이번 포스트에서 SonarQube의 설치과정은 다루지 않습니다.<br>\n즉 이미 SonarQube 서버와, Jenkins 서버가 설치되었다는 가정하에 진행하였습니다.</li>\n</ul>\n<br/>\n<ul>\n<li>설치에 관련된 포스트는 <a href=\"https://www.lesstif.com/software-architect/sonarqube-39126262.html\">여기</a>를 참고해주세요</li>\n</ul>\n<br/>\n<hr>\n<h3 id=\"jenkins-설정-a-namea2a\" style=\"position:relative;\"><a href=\"#jenkins-%EC%84%A4%EC%A0%95-a-namea2a\" aria-label=\"jenkins 설정 a namea2a permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Jenkins 설정 <a name=\"a2\"></a></h3>\n<br/>\n<ul>\n<li>\n<p>Jenkins 내에서 SonarQube를 사용하기 위해서는 아래 플러그인 설치가 필요합니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103322025-cf0d7600-4a7f-11eb-8081-02b118e9b30c.PNG\" alt=\"AAAAAA\"></p>\n<ul>\n<li>자세한 플러그인 정보는 <a href=\"https://plugins.jenkins.io/sonar/\">링크</a> 확인해주세요</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>플러그인 설치가 완료되었다면 Jenkins 환경설정에서 Sonarqube 서버의 설정이 필요합니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103322080-09771300-4a80-11eb-8022-2f2b6e12fd14.PNG\" alt=\"222222\"></p>\n<ul>\n<li>설치한 SonarQube 서버의 정보를 기입해줍니다</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>Jenkins Global tool configuration 탭에서 Scanner에 대한 설정을 합니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103322135-54912600-4a80-11eb-8b21-23d6f51e0fea.PNG\" alt=\"3332131\"></p>\n<ul>\n<li>별다르게 상이하는 부분은 없이 동일하게 설정하면 동작됩니다.</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>파이프라인 프로젝트에서 SonarQube의 변수들을 선언 해줍니다(스크립트의 편의성을 위함)  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103388784-54f7f280-4b4e-11eb-8547-d643c8756523.png\" alt=\"KakaoTalk_20201228_200226180\"></p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103388805-6fca6700-4b4e-11eb-8064-e7896067a891.png\" alt=\"KakaoTalk_20201228_200226346\"></p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>이제 SonarQube 서버에서 Jenkins 서버에 대한 Webhook을 설정합니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103322208-97eb9480-4a80-11eb-8848-bd3142e2bb53.PNG\" alt=\"3333333333\"></p>\n<ul>\n<li>Jenkins 서버의 IP와 Port로 웹훅을 걸어주시면 됩니다.</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>정상적으로 설정이 되었다면 다음과 같이 웹훅이 생성됩니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103322240-b6ea2680-4a80-11eb-9bfe-eb624f8054b9.PNG\" alt=\"AAAAAAAAAAADDDDD\"></p>\n</li>\n</ul>\n<p>이제 Jenkins에서 SonarQube를 사용 할 수 있습니다!!</p>\n<br/>\n<hr>\n<h3 id=\"jenkins-pipeline-script-수정\" style=\"position:relative;\"><a href=\"#jenkins-pipeline-script-%EC%88%98%EC%A0%95\" aria-label=\"jenkins pipeline script 수정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Jenkins Pipeline Script 수정</h3>\n<p>그럼 파이프라인 스크립트 내에 SonarQube와 관련된 내용을 삽입해보겠습니다.</p>\n<ul>\n<li>\n<p>파이프라인 내용</p>\n<div class=\"gatsby-highlight\" data-language=\"cs\"><pre class=\"language-cs\"><code class=\"language-cs\"><span class=\"token function\">properties</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n<span class=\"token function\">parameters</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token keyword\">string</span><span class=\"token punctuation\">(</span><span class=\"token named-parameter punctuation\">name</span><span class=\"token punctuation\">:</span> 'sonar<span class=\"token punctuation\">.</span>projectKey'<span class=\"token punctuation\">,</span> <span class=\"token named-parameter punctuation\">defaultValue</span><span class=\"token punctuation\">:</span> 'com<span class=\"token punctuation\">.</span>appsecco<span class=\"token punctuation\">:</span>dvja'<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">string</span><span class=\"token punctuation\">(</span><span class=\"token named-parameter punctuation\">name</span><span class=\"token punctuation\">:</span> 'sonar<span class=\"token punctuation\">.</span>host<span class=\"token punctuation\">.</span>url'<span class=\"token punctuation\">,</span> <span class=\"token named-parameter punctuation\">defaultValue</span><span class=\"token punctuation\">:</span> 'http<span class=\"token punctuation\">:</span><span class=\"token operator\">/</span><span class=\"token operator\">/</span><span class=\"token number\">34.64</span><span class=\"token number\">.237</span><span class=\"token number\">.112</span><span class=\"token punctuation\">:</span><span class=\"token number\">9000</span>'<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">string</span><span class=\"token punctuation\">(</span><span class=\"token named-parameter punctuation\">name</span><span class=\"token punctuation\">:</span> 'sonar<span class=\"token punctuation\">.</span>login'<span class=\"token punctuation\">,</span> <span class=\"token named-parameter punctuation\">defaultValue</span><span class=\"token punctuation\">:</span> '608cacd6bb83c50712ebb34c4cba377c841cdebb'<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> \n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token range operator\">..</span><span class=\"token punctuation\">.</span></code></pre></div>\n<p>우선 간단하게 파이프라인을 작성하기위해 변수 설정을 했습니다.</p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>그리고 SonarQube와 SonarQube 내에있는 Dependency-Check를 작성해줍니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"cs\"><pre class=\"language-cs\"><code class=\"language-cs\">        stage <span class=\"token punctuation\">(</span>'Dependency<span class=\"token operator\">-</span>Check Analysis'<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n            steps <span class=\"token punctuation\">{</span>\n                sh '<span class=\"token operator\">/</span><span class=\"token keyword\">var</span><span class=\"token operator\">/</span>lib<span class=\"token operator\">/</span>jenkins<span class=\"token operator\">/</span>dependency<span class=\"token operator\">-</span>check<span class=\"token operator\">/</span>bin<span class=\"token operator\">/</span>dependency<span class=\"token operator\">-</span>check<span class=\"token punctuation\">.</span>sh <span class=\"token operator\">--</span>scan `pwd` <span class=\"token operator\">--</span>format XML <span class=\"token operator\">--</span><span class=\"token keyword\">out</span> <span class=\"token operator\">/</span><span class=\"token keyword\">var</span><span class=\"token operator\">/</span>lib<span class=\"token operator\">/</span>jenkins<span class=\"token operator\">/</span>workspace<span class=\"token operator\">/</span>ci<span class=\"token operator\">-</span>build<span class=\"token operator\">-</span>pipeline<span class=\"token operator\">/</span>dependency<span class=\"token operator\">-</span>check<span class=\"token operator\">-</span>report <span class=\"token operator\">--</span>prettyPrint'\n                \n                <span class=\"token class-name\">dependencyCheckPublisher</span> pattern<span class=\"token punctuation\">:</span> 'dependency<span class=\"token operator\">-</span>check<span class=\"token operator\">-</span>report<span class=\"token operator\">/</span>dependency<span class=\"token operator\">-</span>check<span class=\"token operator\">-</span>report<span class=\"token punctuation\">.</span>xml'\n            <span class=\"token punctuation\">}</span>\n        <span class=\"token punctuation\">}</span>\n        <span class=\"token function\">stage</span><span class=\"token punctuation\">(</span>'Sonarqube <span class=\"token keyword\">and</span> Quality gate'<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n            options <span class=\"token punctuation\">{</span>\n                timeout<span class=\"token return-type class-name\"><span class=\"token punctuation\">(</span>time<span class=\"token punctuation\">:</span> 5<span class=\"token punctuation\">,</span> unit<span class=\"token punctuation\">:</span> 'MINUTES'<span class=\"token punctuation\">)</span></span>\n                <span class=\"token function\">retry</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n            <span class=\"token punctuation\">}</span>\n            steps <span class=\"token punctuation\">{</span>\n                <span class=\"token function\">withSonarQubeEnv</span><span class=\"token punctuation\">(</span>'SonarQube Server'<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n                    sh <span class=\"token string\">\"mvn sonar:sonar\"</span>\n                <span class=\"token punctuation\">}</span>\n                script <span class=\"token punctuation\">{</span>\n                    qualitygate <span class=\"token operator\">=</span> <span class=\"token function\">waitForQualityGate</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n                    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>qualitygate<span class=\"token punctuation\">.</span>status <span class=\"token operator\">!=</span> <span class=\"token string\">\"OK\"</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n                        currentBuild<span class=\"token punctuation\">.</span>result <span class=\"token operator\">=</span> <span class=\"token string\">\"FAILURE\"</span>\n                    <span class=\"token punctuation\">}</span>\n                <span class=\"token punctuation\">}</span>\n            <span class=\"token punctuation\">}</span>\n        <span class=\"token punctuation\">}</span></code></pre></div>\n<br/>\n</li>\n</ul>\n<p>여기까지만 하면 파이프라인 내에서는 SonarQube는 정상동작합니다.</p>\n<hr>\n<h3 id=\"파이프라인-실행-결과\" style=\"position:relative;\"><a href=\"#%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EC%8B%A4%ED%96%89-%EA%B2%B0%EA%B3%BC\" aria-label=\"파이프라인 실행 결과 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>파이프라인 실행 결과</h3>\n<ul>\n<li>\n<p>이제 모든 파이프라인 구성이 완료되었습니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103388929-262e4c00-4b4f-11eb-8ff4-ac9d873625eb.PNG\" alt=\"캡처\"></p>\n<ul>\n<li>파이프라인 스크립트의 STAGE별 순서 진행도를 위와같이 확인 할 수 있습니다.</li>\n<li>또한 SonarQube의 분석 결과 그래프도 위와 같이 확인 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>SonarQube의 Check-style 등의 경우 다른 리포트를 생성합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103388989-91781e00-4b4f-11eb-8614-1ebeac419ec4.png\" alt=\"KakaoTalk_20201228_200227145\"></p>\n<ul>\n<li>PASSED의 경우 사용자가 ERROR Level을 임의로 설정 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며…</h2>\n<p>진행한 프로젝트의 보안툴은 모두 마무리 되었습니다.<br>\n미리 미리 정리해놓았던 문서들을 다시 보면서 정리하려고 힘들을 느끼고 있습니다.<br>\n사실 문서화가 가장 중요하다고 생각하지만 업무를 진행하면서 실시간으로 문서화하기는 정말 쉽지 않습니다.<br>\n그래도 블로그 글을 꾸준히 포스트하고 공부하려면 필요 한 일들이니 노력해보겠습니다.  </p>\n<hr>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#-sonarqube-\">✔ SonarQube ??</a></p>\n<ul>\n<li><a href=\"#sonarqube-%EC%84%A4%EC%B9%98\">SonarQube 설치</a></li>\n<li><a href=\"#jenkins-%EC%84%A4%EC%A0%95-a-namea2a\">Jenkins 설정 <a name=\"a2\"></a></a></li>\n<li><a href=\"#jenkins-pipeline-script-%EC%88%98%EC%A0%95\">Jenkins Pipeline Script 수정</a></li>\n<li><a href=\"#%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EC%8B%A4%ED%96%89-%EA%B2%B0%EA%B3%BC\">파이프라인 실행 결과</a></li>\n</ul>\n</li>\n<li><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며…</a></li>\n</ul>\n</div>","excerpt":"머리말   이번 포스트로 이제 파이프라인에서 동작하는 전체적인 보안툴에 대한 포스트는 끝났습니다. 최종적으로는    SonarQube로 Build 될 이미지의 소스코드에 대한 전략적 정적분석을    Anchore로 빌드된 이미지에 대한 분석을   OWASP ZAP으로 배포 된 서비스에 대한 동적분석   위 세가지 보안 항복을 Jenkins를 이용해 자동화 하였습니다. 사용 할 툴을 다음과 같습니다.   Jenkins Sonarqube ✔ SonarQube ?? 위키백과 왈 소나큐브(SonarQube, 이전 이름: 소나/Sonar)는 20개 이상의 프로그래밍 언어에서 버그, 코드 스멜, 보안 취약점을 발견할 목적으로 정…","frontmatter":{"date":"August 12, 2021","title":"[DEVOPS] - SonarQube With Jenkins","categories":"DevOps","author":"nasa1515","emoji":"🤦‍♂️"},"fields":{"slug":"/devops-sonarqube/"}},"next":{"id":"db35f671-1882-5449-b83f-40eecad72930","html":"<p>머리말  </p>\n<p>지난 포스트에서 간단하게 전체적인 파이프라인에 대해서 포스트를 했습니다.<br>\n이번 포스트는 Harbor에 배포 될 Container Image 분석 오픈소스 Anchore를 도입했던 포스트를 작성했습니다.</p>\n<hr>\n<ul>\n<li>\n<p>사용 할 툴을 다음과 같습니다.  </p>\n<ul>\n<li>Jenkins</li>\n<li>Anchore</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"-anchore-\" style=\"position:relative;\"><a href=\"#-anchore-\" aria-label=\" anchore  permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✔ Anchore ??</h2>\n<p>정말 간단히 설명해서 Docker Image의 취약점을 스캔하는 스캐너라고 생각하면 됩니다.</p>\n<p>Anchore 오픈 소스 버전은 다음에서 참고할 수 있습니다.</p>\n<p><a href=\"https://anchore.com/opensource/\">https://anchore.com/opensource/</a></p>\n<br/>\n<ol>\n<li>이미지 분석<br>\n컨테이너 이미지의 심층 검사를 수행하여 모든 OS의 패키지, 파일 및 소프트웨어 아티팩트<br>\n(Ruby GEMs, JARs, Node Modules) Cataloging화 한다.  </li>\n</ol>\n<br/>\n<ol start=\"2\">\n<li>정책 관리<br>\n보안 모범 사례를 기반으로 정책을 정의하고 적용하여 위험한 빌드가 완료되지 않고 문제가 있는 이미지가 배포되지 않도록 한다.  </li>\n</ol>\n<br/>\n<ol start=\"3\">\n<li>Continuous Monitoring<br>\n이미지가 업데이트되거나 CVE가 추가 또는 제거되거나 새로운 모범 사례가 설정 될 때<br>\n생성 된 문제를 파악하기 위해 정책을 지속적으로 관리한다.</li>\n</ol>\n<br/>\n<ol start=\"4\">\n<li>CI / CD 통합<br>\nAnchore Engine을 CI/CD 파이프 라인에 통합하여<br>\n이미지가 사용자 지정 보안 및 요구 사항을 충족할 때만 성공적으로 빌드되도록한다.</li>\n</ol>\n<br/>\n<ol start=\"5\">\n<li>커스터마이징<br>\n이미지 내부 Package, Whitelists, Blacklists, 설정파일, 보안, Manifest, 포트 등에<br>\n대한 취약점을 점검하기 정책을 유연하게 정의할 수 있다.</li>\n</ol>\n<br/>\n<hr>\n<h3 id=\"anchore-설치-docker\" style=\"position:relative;\"><a href=\"#anchore-%EC%84%A4%EC%B9%98-docker\" aria-label=\"anchore 설치 docker permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Anchore 설치 (Docker)</h3>\n<br/>\n<p>Anchore 설치와 Jenkins와 연동은 이미 많은 분들이 포스트하셨습니다.<br>\n따라서 제 포스트에서는 간단하게 명령어, 이미 설치되어있다는 가정하에 정리하였습니다.</p>\n<ul>\n<li>\n<p>설치 명령어</p>\n<div class=\"gatsby-highlight\" data-language=\"cs\"><pre class=\"language-cs\"><code class=\"language-cs\">$ <span class=\"token class-name\">curl</span> https<span class=\"token punctuation\">:</span><span class=\"token operator\">/</span><span class=\"token operator\">/</span>docs<span class=\"token punctuation\">.</span>anchore<span class=\"token punctuation\">.</span>com<span class=\"token operator\">/</span>current<span class=\"token operator\">/</span>docs<span class=\"token operator\">/</span>engine<span class=\"token operator\">/</span>quickstart<span class=\"token operator\">/</span>docker<span class=\"token operator\">-</span>compose<span class=\"token punctuation\">.</span>yaml <span class=\"token operator\">></span> docker<span class=\"token operator\">-</span>compose<span class=\"token punctuation\">.</span>yaml\n$ docker<span class=\"token operator\">-</span>compose up <span class=\"token operator\">-</span>d\n\n$ yum install epel<span class=\"token operator\">-</span>release\n$ yum install python<span class=\"token operator\">-</span>pip\n$ pip install anchorecli</code></pre></div>\n<hr>\n</li>\n</ul>\n<p>추가적으로 anchore를 jenkins에서 사용하기 위해서는 plugin을 설치해야 합니다.  </p>\n<ul>\n<li>\n<p>설치될 plugin은 다음과 같습니다.</p>\n<ul>\n<li><a href=\"http://plugins.jenkins.io/anchore-container-scanner/\">http://plugins.jenkins.io/anchore-container-scanner/</a>  </li>\n</ul>\n</li>\n</ul>\n<p>plugin설치가 완료된후 jenkins의 configuration system 메뉴에서<br>\nAnchore Container Image Scanner 설정을 추가합니다.  (아래사진)</p>\n<ul>\n<li>\n<p>Jenkins에서 동작하는 Anchore는 다음과 같은 WorkFlow를 가집니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103251164-e59dc980-49ba-11eb-871f-ecfb62bdad91.PNG\" alt=\"3332112\"></p>\n</li>\n</ul>\n<br/>\n<hr>\n<h3 id=\"gcp-방화벽-설정\" style=\"position:relative;\"><a href=\"#gcp-%EB%B0%A9%ED%99%94%EB%B2%BD-%EC%84%A4%EC%A0%95\" aria-label=\"gcp 방화벽 설정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GCP 방화벽 설정</h3>\n<p>GCP 기반의 인프라이기 때문에 GCP의 VPN에서 설치 할 때 설정해줬던<br>\nAnchore의 Service Port를 허용해줘야만 Jenkins에서 연동이 가능합니다.</p>\n<ul>\n<li>\n<p>다음과 같이 Jenkins-환경설정에서 Anchore Container Image Scanner<br>\n설정의 Servic Port를 Anchore와 GCP에서 Allow 해준 Port를 기입하면 됩니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103250454-3b707280-49b7-11eb-95ce-220663b7dc54.PNG\" alt=\"캡처22\"></p>\n</li>\n</ul>\n<br/>\n<hr>\n<h3 id=\"jenkins-pipeline-script-수정\" style=\"position:relative;\"><a href=\"#jenkins-pipeline-script-%EC%88%98%EC%A0%95\" aria-label=\"jenkins pipeline script 수정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Jenkins Pipeline Script 수정</h3>\n<p>위에 있는 연동을 위한 환경설정들이 모두 마무리 되었으면<br>\n아래처럼 파이프라인 스크립트내에 Anchore의 Analyse부분을 추가해줍니다.</p>\n<ul>\n<li>\n<p>스크립트  </p>\n<div class=\"gatsby-highlight\" data-language=\"cs\"><pre class=\"language-cs\"><code class=\"language-cs\">        <span class=\"token function\">stage</span><span class=\"token punctuation\">(</span>'Anchore analyse'<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>  \n            steps <span class=\"token punctuation\">{</span>  \n                <span class=\"token function\">catchError</span><span class=\"token punctuation\">(</span><span class=\"token named-parameter punctuation\">buildResult</span><span class=\"token punctuation\">:</span> 'SUCCESS'<span class=\"token punctuation\">,</span> <span class=\"token named-parameter punctuation\">stageResult</span><span class=\"token punctuation\">:</span> 'SUCCESS'<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n                <span class=\"token class-name\">writeFile</span> file<span class=\"token punctuation\">:</span> 'anchore_images'<span class=\"token punctuation\">,</span> <span class=\"token named-parameter punctuation\">text</span><span class=\"token punctuation\">:</span> '<span class=\"token number\">34.64</span><span class=\"token number\">.237</span><span class=\"token number\">.112</span><span class=\"token operator\">/</span>cccr<span class=\"token operator\">/</span>jisun'  \n                <span class=\"token class-name\">anchore</span> name<span class=\"token punctuation\">:</span> 'anchore_images'  \n                <span class=\"token punctuation\">}</span>\n            <span class=\"token punctuation\">}</span>\n        <span class=\"token punctuation\">}</span></code></pre></div>\n</li>\n</ul>\n<br/>\n<hr>\n<h3 id=\"파이프라인-실행-결과\" style=\"position:relative;\"><a href=\"#%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EC%8B%A4%ED%96%89-%EA%B2%B0%EA%B3%BC\" aria-label=\"파이프라인 실행 결과 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>파이프라인 실행 결과</h3>\n<p>제대로 연동되었다면 파이프라인이 종료된 뒤 가시적인 로그를 볼 수 있습니다.</p>\n<ul>\n<li>\n<p>Anchore 스캐닝 리포트</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103250671-5f808380-49b8-11eb-8ef1-c939886fde60.PNG\" alt=\"33\"></p>\n</li>\n</ul>\n<br/>\n<hr>\n<h3 id=\"정상-스캐닝-여부-확인\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EC%83%81-%EC%8A%A4%EC%BA%90%EB%8B%9D-%EC%97%AC%EB%B6%80-%ED%99%95%EC%9D%B8\" aria-label=\"정상 스캐닝 여부 확인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정상 스캐닝 여부 확인</h3>\n<p>Anchore의 가시적인 리포트는 확인 할 수 있지만 정말 정확한 스캐닝을 하는 건지는 잘 모르겠습니다.<br>\n그래서 나온 ERROR 중 일부를 수정해서 결과가 반영되는지를 확인해보죠</p>\n<ul>\n<li>\n<p>Anchore의 리포트를 보니 아래와 같은 이슈가 있었습니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103250811-1c72e000-49b9-11eb-9b27-ab7849ec81c5.PNG\" alt=\"44\"></p>\n<p>로그확인을 해보니 - Oracle MySQL 5.7.14 이상으로 버전 업그레이드 필요하다는 로그였네요</p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>바로 관련된 소스를 수정합니다!</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103250871-6fe52e00-49b9-11eb-9716-7c3fb7eaff84.PNG\" alt=\"aaaa\"></p>\n<p>간단하게 연결되어있는 DVWA 앱의 pom.xml의 소스를 수정해서 반영시켜봤습니다.</p>\n</li>\n</ul>\n<br/>\n<p>수정해서 파이프라인을 동작시키니 정상적으로 스캐닝 하고있음을 확인했습니다.</p>\n<ul>\n<li>\n<ol>\n<li>Anchore 리포트 로그의 MYSQL 항목을 더 이상 찾을 수 없습니다.</li>\n</ol>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103250938-cb172080-49b9-11eb-8909-3c27a7901c58.PNG\" alt=\"dds\"></p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<ol start=\"2\">\n<li>가시적인 그래프가 줄어 들었음을 확인 할 수 있습니다.</li>\n</ol>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/103250982-fe59af80-49b9-11eb-8f6c-6ae4a33c9ece.PNG\" alt=\"ssssss\"></p>\n<p>다음과 같이 마지막 Build에서 그래프가 꺾여 내려갑니다.</p>\n</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며…</h2>\n<p>조금씩 프로젝트 포스트에서 막바지가 다가와 갑니다.<br>\nAnchore는 비교적 쉽게 구현했던 오픈소스입니다.<br>\n그나마 조금 시간을 잡아먹거나, 불편했던 점이라고 한다면\nAnchore는 기본적으로 파이프라인의 성공/실패 여부에 따라<br>\nBuild를 멈추거나 무시하도록 설정 할 수 있게 되어있는데 이 부분을 모르고 있었습니다.<br>\n계속해서 Anchore가 실행 될 때마다 Build가 멈추는 현상이 일어나 원인분석에 2일정도를 썼던 것 같습니다..<br>\n정리를 해가면서 다시 한번 배우고 있는 것 같습니다.<br>\n한국은 아직 Anchore 툴의 여부를 모르는 사람들도 많은 것 같은데 하루 빨리 많은 오픈소스들이 산업에 도입되었으면..</p>\n<p>다음글은 Sornaqube 포스트로 돌아 오겠습니다.</p>\n<hr>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#-anchore-\">✔ Anchore ??</a></p>\n<ul>\n<li><a href=\"#anchore-%EC%84%A4%EC%B9%98-docker\">Anchore 설치 (Docker)</a></li>\n<li><a href=\"#gcp-%EB%B0%A9%ED%99%94%EB%B2%BD-%EC%84%A4%EC%A0%95\">GCP 방화벽 설정</a></li>\n<li><a href=\"#jenkins-pipeline-script-%EC%88%98%EC%A0%95\">Jenkins Pipeline Script 수정</a></li>\n<li><a href=\"#%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EC%8B%A4%ED%96%89-%EA%B2%B0%EA%B3%BC\">파이프라인 실행 결과</a></li>\n<li><a href=\"#%EC%A0%95%EC%83%81-%EC%8A%A4%EC%BA%90%EB%8B%9D-%EC%97%AC%EB%B6%80-%ED%99%95%EC%9D%B8\">정상 스캐닝 여부 확인</a></li>\n</ul>\n</li>\n<li><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며…</a></li>\n</ul>\n</div>","frontmatter":{"date":"August 11, 2021","title":"[DEVOPS] - 이미지 분석 툴 Anchore With Jenkins","categories":"DevOps","author":"nasa1515","emoji":"🤦‍♂️"},"fields":{"slug":"/devops-anchor/"}},"prev":{"id":"2a5bcac3-907a-58f0-81da-8f92654bc3e6","html":"<p>머리말  </p>\n<p>이번에는 데이터의 가장 기초적인 오픈소스인 Apache Spark에 대한 내용 정리입니다.<br>\n아무것도 모르는 생짜 초보이기 때문에 틀린 부분이 많을 수 있습니다.  </p>\n<hr>\n<h2 id=\"-apache-spark-hadoop\" style=\"position:relative;\"><a href=\"#-apache-spark-hadoop\" aria-label=\" apache spark hadoop permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✔ Apache Spark? Hadoop?</h2>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109732527-b3018e80-7c00-11eb-8fc9-53e9618bfac5.JPG\" alt=\"캡처1\"></p>\n<p>주워들은 말로는 데이터 시장은 오픈소스인 Hadoop과 Apache가 경쟁하며 성장하고 있다고 알고 있다<br>\n그런데 또 다른 글들을 보니 이미 업계에서는 두 오픈소스를 동시에 사용한다고도 한다.<br>\n경쟁하는 관계인데 또 상생을 하고 있다는게 무슨소리지?<br>\n다시 한번 찾아보니 각각의 툴의 용도에 대해서 알지 못했던 나의 오착이었다.  </p>\n<br/>\n<p>내가 이해한 두 앱의 용도를 간단하게 설명해보면<br>\n우선 두 툴은 빅데이터 처리 플랫폼, 프레임워크라는 공통점을 가지고 있지만 </p>\n<ul>\n<li>\n<h3 id=\"hadoop\" style=\"position:relative;\"><a href=\"#hadoop\" aria-label=\"hadoop permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Hadoop</h3>\n<p>분산 데이터 Infrastructure를 주로 하며,<br>\n대량의 데이터를 Server Cluster 내 복수 노드들에 분산시키는 역할을 한다.<br>\n이를 통해 데이터 처리를 위한 필요한 하드웨어의 비용부담을 줄여준다.   </p>\n</li>\n</ul>\n<p>반면에 Spark는 분산 데이터 컬렉션 상부에서 동작하는 <code class=\"language-text\">데이터 프로세싱 툴</code>로<br>\n분산형 스토리지의 역할은 수행하지 않는다고 한다.<br>\n대충 이 대목에서 왜 두 오픈소스를 상생하면서 쓰는지 감이오기 시작했다.  </p>\n<p>Hadoop은 HDFS(Hadoop Database filesystem)을 사용하며 맵리듀스를 핵심 구성 요소로 제공한다.  따라서 Spark가 없어도 된다.<br>\n반대로 Spark도 HDFS가 아닌 AWS,GCP,Azure 등과 융합될 수 있기에 Hadoop이 없어도 된다.<br>\n그러나 Hadoop과 Spark를 같이 사용할때가 가장 적합하다고 한다.  </p>\n<br/>\n<p>두 툴의 확실한 차이는 속도에서 확인이 가능하다. </p>\n<p>일반적인 상황에서 Hadoop보다 스파크의 속도가 월등히 빠르다고 한다.<br>\n이유는 데이터 프로세싱 절차의 차이 때문인데<br>\nHadoop은 MapReduce를 사용하기 때문이고, Spark는 DataSet 전체를 한번에 다루기 때문에…<br>\n또한 아래에서 다시 설명하겠지만 Hadoop은 HW에서, Spark는 메모리에서 동작하기 때문이다..  </p>\n<br/>\n<ul>\n<li>\n<p>Hadoop의 Mapreduce WorkFlow  </p>\n<p>Input -> Splitting -> Mapping -> Shuffling -> Reducing -> Final Result</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109735558-4ee1c900-7c06-11eb-85aa-5fd05dc011f1.jpg\" alt=\"99F6AA445B5975A320\"></p>\n<p><em>INPUT : 먼저 클러스터에서 데이터를 읽고</em><br>\n<em>클라이언트->네임노드->클라이언트->데이터 노드-> 마스터(Job Tracker)</em><br>\n<em>태스크 단위로 쪼개어 Tasketracker(worker)에 배정하고</em><br>\n<em>Map 단계를 수행한 후, 중간 결과물을 로컬 디스크에 저장을 한다.</em><br>\n<em>그리고 그 결과물을 다시 combine, partioning을 거쳐 나온 2차 중간 결과물을 디스크에 분할 저장한다.</em><br>\n<em>그리고 최종적으로 shuffling을 통해 reduce 작업에 할당된 후</em><br>\n<em>reduce 작업을 거쳐 최종적으로 나온 결과물이 HDFS에 저장된다”.</em></p>\n</li>\n</ul>\n<br/>\n<p>이에 반해, 스파크는 모든 데이터 운영을 메모리 내에서 실시간에 가깝게 처리할 수 있다(인메모리).<br>\n데이터를 읽고, 처리 분석을 거친 결과물을 클러스터에 입력하는 전 과정이 동시에 진행되는 것이다.<br>\n배치 프로세싱 경우에 스파크가 10배 빠르고, 인 메모리 Analytics의 경우, 100배 빠르다고 알려져있다.   </p>\n<br/>\n<p>나는 여기서 왜 Spark가 더 좋은데 Hadoop을 쓰지? 라는 의문이 들었다.<br>\n그러나 대부분의 Data 운영, 리포팅 요구의 대부분이 정적인 것들이고 시간의 여유가 있다면 Mapreduce의 방식을 채택한다고 한다.<br>\n다만 Spark가 필수적으로 필요할 때가 있는데 이는 비즈니스 공장의 센서 등 실시간으로 수집되는 스트리밍 데이터를 처리하거나, ML 알고리즘과 같이 APP의 복합적인 운영을 할때라고 한다.<br>\n그리고 애초에 Hadoop만 사용하다가 위와 같이 실시간 적인 데이터 처리를 위해서 도입한 것이<br>\nSpark라서 그냥 두 툴을 같이 쓰는게 최적이라고 한다.  </p>\n<br/>\n<hr>\n<h2 id=\"-apache-spark\" style=\"position:relative;\"><a href=\"#-apache-spark\" aria-label=\" apache spark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✌ Apache Spark</h2>\n<p>그럼 간단하게 데이터 플랫폼 2개의 툴에 대해서 설명했으니<br>\n오늘 포스트의 주제인 Spark에 대한 내용으로 돌아와보자 </p>\n<ul>\n<li>\n<h3 id=\"spark의-구성-요소\" style=\"position:relative;\"><a href=\"#spark%EC%9D%98-%EA%B5%AC%EC%84%B1-%EC%9A%94%EC%86%8C\" aria-label=\"spark의 구성 요소 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark의 구성 요소</h3>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109738566-61aacc80-7c0b-11eb-9c66-5f50dff0e63b.jpg\" alt=\"components_of_spark\"></p>\n<p>Spark는 다음 그림과 같은 구성 요소를 가지고 있습니다.</p>\n</li>\n</ul>\n<h3 id=\"apache-spark-core\" style=\"position:relative;\"><a href=\"#apache-spark-core\" aria-label=\"apache spark core permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Apache Spark Core</h3>\n<ul>\n<li>Spark job과 다른 Spark 컴포넌트에 필요한 기본 기능을 제공합니다.  </li>\n<li>주로 분산 데이터 컬렉션(DataSet)을 추상화한 객체 RDD로 다양한 연산, 변환 메소드를 제공합니다.</li>\n<li>HDFS, GlusterFS, S3등 여러 Filsystem에 접근이 가능합니다.  </li>\n<li>공유 변수, 누적 변수를 사용해 컴퓨팅 노드 간 정보를 공유합니다.  </li>\n<li>Spark core에는 네트워킹, 보안, 스케쥴링 및 데이터 셔플링 등 기본 기능을 제공합니다.  </li>\n</ul>\n<h3 id=\"spark-sql\" style=\"position:relative;\"><a href=\"#spark-sql\" aria-label=\"spark sql permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark SQL</h3>\n<ul>\n<li>Spark와 하이브 SQL이 지원하는 SQL을 사용해 대규모 분산 정형 데이터를 다룰 수 있습니다.  </li>\n<li>JSON File, Parquet 파일, RDB 테이블, 하이브 테이블 등 여러 정형 데이터를 읽고 쓸 수 있습니다.  </li>\n<li>DataFrame 과 DataSet의 연산을 RDD 연산으로 변환해 일반 Spark job으로 실행.  </li>\n</ul>\n<h3 id=\"spark-streaming\" style=\"position:relative;\"><a href=\"#spark-streaming\" aria-label=\"spark streaming permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Streaming</h3>\n<ul>\n<li>실시간 스트리밍 데이터를 처리하는 프레임 워크.  </li>\n<li>HDFS, Kafka, Flume, 트위터 등 커스텀 리소스도 사용 가능합니다.</li>\n<li>다른 Spark 컴포넌트 겸용, 실시간 데이터 처리를 ML, SQL, Graph와 통합 연산이 가능.  </li>\n</ul>\n<h3 id=\"spark-mllib\" style=\"position:relative;\"><a href=\"#spark-mllib\" aria-label=\"spark mllib permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark MLlib</h3>\n<ul>\n<li>머신 러닝 알고리즘 라이브러리.</li>\n<li>RDD, DataFrame의 DataSet을 변환하는 머신 러닝 모델을 구현 가능.  </li>\n</ul>\n<h3 id=\"spark-graphx\" style=\"position:relative;\"><a href=\"#spark-graphx\" aria-label=\"spark graphx permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark GraphX</h3>\n<ul>\n<li>그래프 RDD 형태의 그래프 구조를 만들 수 있는 기능을 제공.</li>\n</ul>\n<br/>\n<h3 id=\"spark-cluster의-구조와-실행과정\" style=\"position:relative;\"><a href=\"#spark-cluster%EC%9D%98-%EA%B5%AC%EC%A1%B0%EC%99%80-%EC%8B%A4%ED%96%89%EA%B3%BC%EC%A0%95\" aria-label=\"spark cluster의 구조와 실행과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Cluster의 구조와 실행과정</h3>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109743000-5491db80-7c13-11eb-9d18-516463788a2a.png\" alt=\"다운로드\"></p>\n<ul>\n<li>Spark Application은 실제 작업을 수행하는 역할이고  </li>\n<li>Cluster Manager는 Application 사이에 자원을 중계해주는 역할을 담당합니다.  </li>\n</ul>\n<br/>\n<h3 id=\"spark-application\" style=\"position:relative;\"><a href=\"#spark-application\" aria-label=\"spark application permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Application</h3>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109753108-ff5ec580-7c24-11eb-989a-fd120dee21ad.png\" alt=\"다운로드\"></p>\n<p>Spark Application은 Driver 프로세스와 Excutors 두개로 구성됩니다.  </p>\n<ul>\n<li>Spark Driver (Master) : 한개의 노드에서만 실행되고, Spark 전체의 main()함수를 실행합니다.<br>\nApplication 내 정보의 유지관리, Excutors의 실행 및 실행 분석, 배포 등 Master의 역할을 수행합니다.<br>\n즉 간단하게 사용자가 구성한 JOB을 TASK 단위로 변환해 Executor로 전달합니다.  </li>\n</ul>\n<br/>\n<ul>\n<li>Executer (Worker Node) : 다수의 Worker Node에서 실행되는 프로세스<br>\nMaster(Spark Driver)가 할당한 작업(TASK)를 수행한 결과를 반환.<br>\n추가로 블록매니저를 통해서 Cache하는 RDD를 저장합니다.  </li>\n</ul>\n<br/>\n<h3 id=\"cluster-manager\" style=\"position:relative;\"><a href=\"#cluster-manager\" aria-label=\"cluster manager permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cluster Manager</h3>\n<p>이름 그대로 Spark Application의 Resource를 효율적으로 분배하는 역할을 담당합니다.<br>\n바로 위에서 Driver에서 Executors로 task를 할당하고 관리한다고 설명했는데<br>\n그 작업을 진행하기 위해 Clouster Mananger에 의존하고 있습니다.(없어선안됨…)<br>\n즉 TASK의 할당 및 관리는 Driver -> Executors 구조가 아니라<br>\nDriver &#x3C;-> Cluster Manager &#x3C;-> Executors 구조 입니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109754173-e820d780-7c26-11eb-99e7-05e46796c3d8.JPG\" alt=\"222\"></p>\n<p>현재 Spark 3.0 기준 대표적인 Cluster Manager의 종류는 위 3가지 +  Spark StandAlone 입니다.<br>\n근데 대부분 YARN,k8s 두 종류만 사용하는 듯…?  </p>\n<p>다른 Manager는 이해가 되는데 StandAlone은 무슨말일까..?  </p>\n<ul>\n<li>StandAlone\nSpark StandAlone은 Cluster로 구성하지 않고 단일 컴퓨터에서 동작시키는 거였다.<br>\n원래라면 나눠져야 할 Driver와 Executor는 각각 Thread로 동작한다고 한다.<br>\nCluster로 구성한다면 Worker Node에 여러개의 Executor를 실행 시킬 수 있지만 StandAlone의 경우 1개씩만 동작한다.  </li>\n</ul>\n<br/>\n<hr>\n<h3 id=\"spark-apis\" style=\"position:relative;\"><a href=\"#spark-apis\" aria-label=\"spark apis permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark APIs</h3>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109767953-9c2c5d80-7c3b-11eb-914f-00aee9d23e95.png\" alt=\"3rF6p\"></p>\n<p>Spark Application은 v1 ~ v3를 거쳐 다음과 같이 3가지의 APIs를 사용합니다.  </p>\n<h4 id=\"rdd-resillient-distributed-dataset\" style=\"position:relative;\"><a href=\"#rdd-resillient-distributed-dataset\" aria-label=\"rdd resillient distributed dataset permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RDD (Resillient Distributed DataSet)</h4>\n<p>RDD는 이름을 그대로 풀어쓰면 이해하기가 쉽습니다.  </p>\n<ul>\n<li>Resillient : Mem 내 데이터 손실 시 다시 생성이 가능하다</li>\n<li>Distributed : Cluster를 통해 메모리에 분산되어서 저장된다 (분산) </li>\n<li>DataSet : 파일을 통해 가져 올 수 있다. 변경되지 않는다.  </li>\n</ul>\n<p>정리하면 여러 분산 노드에 걸쳐서 저장되는 변경이 불가능한 데이터의 집합입니다.   </p>\n<p>RDD의 생성은 2가지 방법으로서 생성됩니다.  </p>\n<ul>\n<li>외부로 부터 Data를 로딩할때 (Disk)</li>\n<li>코드에서 생성된 Data를 저장할 때</li>\n</ul>\n<p>추가적으로 RDD에서 제공하는 Operations(function) 역시 2가지만 존재합니다.  </p>\n<ul>\n<li>\n<h4 id=\"transformation-변환--존재하는-rdd에서-새로운-rdd를-생성하는-함수\" style=\"position:relative;\"><a href=\"#transformation-%EB%B3%80%ED%99%98--%EC%A1%B4%EC%9E%AC%ED%95%98%EB%8A%94-rdd%EC%97%90%EC%84%9C-%EC%83%88%EB%A1%9C%EC%9A%B4-rdd%EB%A5%BC-%EC%83%9D%EC%84%B1%ED%95%98%EB%8A%94-%ED%95%A8%EC%88%98\" aria-label=\"transformation 변환  존재하는 rdd에서 새로운 rdd를 생성하는 함수 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transformation (변환) : 존재하는 RDD에서 새로운 RDD를 생성하는 함수</h4>\n<ul>\n<li>예를 들어 {1,2,3,3} 의 값을 가진 RDD에 Transformation을 사용하면  </li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109771135-d0a21880-7c3f-11eb-841a-287b875cb201.JPG\" alt=\"캡처222\"></p>\n<ul>\n<li>추가적으로 {1,2,3},{3,4,5} 두 값을 가진 RDD의 경우 </li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109771264-f4655e80-7c3f-11eb-80d5-0ee52f6f90dc.JPG\" alt=\"캡처333\"></p>\n<p>그림을 보면 이해가 쉬울 것이다. 대충 RDD의 데이터를 가지고 사용하는 작업이니… </p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<h4 id=\"action-액션--실제로-job을-실행하는-함수-값을-받아오거나-저장한다\" style=\"position:relative;\"><a href=\"#action-%EC%95%A1%EC%85%98--%EC%8B%A4%EC%A0%9C%EB%A1%9C-job%EC%9D%84-%EC%8B%A4%ED%96%89%ED%95%98%EB%8A%94-%ED%95%A8%EC%88%98-%EA%B0%92%EC%9D%84-%EB%B0%9B%EC%95%84%EC%98%A4%EA%B1%B0%EB%82%98-%EC%A0%80%EC%9E%A5%ED%95%9C%EB%8B%A4\" aria-label=\"action 액션  실제로 job을 실행하는 함수 값을 받아오거나 저장한다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Action (액션) : 실제로 JOB을 실행하는 함수, 값을 받아오거나 저장한다.</h4>\n<ul>\n<li>예를 들어 {1,2,3,3} RDD에서 Action 함수를 사용하면  </li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109771572-502fe780-7c40-11eb-8c9c-8cd33e05c808.JPG\" alt=\"캡처4444\"></p>\n<p>reduce 함수를 예로 들면 rdd.reduce(x,y: x+y) 이다.<br>\n그럼 RDD {1,2,3,3}의 값들이 각각 x,y가 되어 합한 1+2+3+3 = 9가 연산 후 반환된다.  </p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>간단하게 위의 Flow를 요악한 그림</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/109773128-2d063780-7c42-11eb-9872-47784860e468.png\" alt=\"다운로드33333\"></p>\n</li>\n</ul>\n<p>추가적인 함수의 경우 다른 포스트에서 정리 할 예정입니다.!!</p>\n<br/>\n<h4 id=\"dataframe\" style=\"position:relative;\"><a href=\"#dataframe\" aria-label=\"dataframe permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataFrame</h4>\n<p>Spark v1.3 이후 부터 RDD에서 발전한 개념?<br>\n기존의 RDD의 단점들 속도, 최적화 등을 보완하였다고 합니다.<br>\n여기저기서 찾아본 것으로는 scala에서 다음과 같이 사용이 가능하다고 한다.  </p>\n<p>val df = spark.sql(“실행 Query”)  </p>\n<p>대충 이해하자면 스키마의 최적화부분? 비정형 dataset으로 이해했던 RDD와 다르게 정형 데이터 (테이블)식으로 처리하는 듯 하다.(SQL 사용가능)<br>\n근데 DataSet이 있어서 DataFrame은 완벽히 이해하고 넘어가지 않아도 될 듯??  </p>\n<br/>\n<h4 id=\"dataset\" style=\"position:relative;\"><a href=\"#dataset\" aria-label=\"dataset permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataSet</h4>\n<p>Spark v1.6에서 추가되었다고 합니다.<br>\n데이터 타입체크, 직렬화를 위한 인코더, 카탈리스트 옵티마이저를 지원하고<br>\n데이터 처리 속도를 더욱 증가시켰다고 하는데;;; 잘모르겠다..<br>\nSpark v2.0에서 DataFrame + Dataset = Dataset으로 통합되었다고 하고 따로 DataFrame을 선언하는 느낌인듯 합니다.  </p>\n<br/>\n<hr>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며…</h2>\n<p>Spark의 기본 개념의 50%정도는 이해한 것 같습니다.<br>\n다음 포스트에서는 마저 정리 못한 APIs와 JOB, STAGE, TASK에 대해서 정리 예정입니다. </p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#-apache-spark-hadoop\">✔ Apache Spark? Hadoop?</a></li>\n<li>\n<p><a href=\"#-apache-spark\">✌ Apache Spark</a></p>\n<ul>\n<li><a href=\"#apache-spark-core\">Apache Spark Core</a></li>\n<li><a href=\"#spark-sql\">Spark SQL</a></li>\n<li><a href=\"#spark-streaming\">Spark Streaming</a></li>\n<li><a href=\"#spark-mllib\">Spark MLlib</a></li>\n<li><a href=\"#spark-graphx\">Spark GraphX</a></li>\n<li><a href=\"#spark-cluster%EC%9D%98-%EA%B5%AC%EC%A1%B0%EC%99%80-%EC%8B%A4%ED%96%89%EA%B3%BC%EC%A0%95\">Spark Cluster의 구조와 실행과정</a></li>\n<li><a href=\"#spark-application\">Spark Application</a></li>\n<li><a href=\"#cluster-manager\">Cluster Manager</a></li>\n<li>\n<p><a href=\"#spark-apis\">Spark APIs</a></p>\n<ul>\n<li><a href=\"#rdd-resillient-distributed-dataset\">RDD (Resillient Distributed DataSet)</a></li>\n<li><a href=\"#dataframe\">DataFrame</a></li>\n<li><a href=\"#dataset\">DataSet</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며…</a></li>\n</ul>\n</div>","frontmatter":{"date":"August 13, 2021","title":"[DATA] - Apache Spark란??","categories":"DATA","author":"nasa1515","emoji":"🤦‍♂️"},"fields":{"slug":"/date-spark/"}},"site":{"siteMetadata":{"siteUrl":"https://nasa1515.com","comments":{"utterances":{"repo":"nasa1515/nasablog"}}}}},"pageContext":{"slug":"/devops-sonarqube/","nextSlug":"/devops-anchor/","prevSlug":"/date-spark/"}},"staticQueryHashes":["1073350324","2938748437"]}