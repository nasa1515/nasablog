{"componentChunkName":"component---src-templates-blog-template-js","path":"/data-databricks/","result":{"data":{"cur":{"id":"14668db0-2f50-511a-abf9-9d92fcdc88bd","html":"<br/>\n<p>머리말  </p>\n<p>저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다.<br>\n이번에는 GCP에서 파트너 SaaS형태로 제공해주는 DataBricks를 사용해서<br>\n지난번과 동일한 데이터, 스트립트를 이용해서 성능이나, 사용법에 대한 테스트를 해봤습니다.<br>\n물논 이번에도 파이썬을 첨가해서  </p>\n<hr>\n<h2 id=\"-databricks\" style=\"position:relative;\"><a href=\"#-databricks\" aria-label=\" databricks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✔ DataBricks?</h2>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117228331-cf65b580-ae53-11eb-9b9d-81bd0a524677.png\" alt=\"og-databricks\"></p>\n<p>Databricks란?<br>\n간단 요약해서 Spark,Hadoop 등 빅데이터 관련 솔루션 실행환경을 제공하는 클라우드 서비스입니다.<br>\n통합 분석 플랫폼으로, 한 WorkSpace내에서 여러 서비스를 사용해 모든 분석이 가능합니다.<br>\n이전에 Spark, Hadoop을 ON-Premise 환경에 설치한 포스트를 확인해보시면 알겠지만<br>\nJDK부터 연동해야 하는 부분이 매우 귀찮고 오랜 시간이 걸리게 됩니다.<br>\nDataBricks를 사용하면 설치, 설정 부분 없이 바로 사용이 가능하다는 장점이 있습니다.  </p>\n<p>DataBricks는 아래 작업들을 한 WorkSpace에서 지원합니다.  </p>\n<ul>\n<li>reports</li>\n<li>dashboards</li>\n<li>ETL 작업 실행 (Extract, Transform, Load)</li>\n<li>머신러닝, 스트림 작업</li>\n<li>아파치 Spark보다 더 optimized.</li>\n<li>Databricks 서버와 실시간으로 interaction</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-gcp-databricks\" style=\"position:relative;\"><a href=\"#-gcp-databricks\" aria-label=\" gcp databricks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👌 GCP DataBricks?</h2>\n<p>GCP, Azure, AWS 등 3사 Public Cloud는 이미 DataBricks를 SaaS, PaaS 형태로 지원하고 있습니다.<br>\nGCP의 경우 아직 도입된지 1년이 채 안되서 불안정한 부분도 있고 Korea Region도 지원하지 않습니다.<br>\n아직 GA일정도 나오지 않은 상태구요…(AWS,AZURE는 다 있는데…)<br>\n그래서 이번 포스트에서는 어쩔 수 없이 US Region에서의 테스트를 진행하겠습니다.  </p>\n<br/>\n<ul>\n<li>\n<h4 id=\"databricks-사용하기구독\" style=\"position:relative;\"><a href=\"#databricks-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0%EA%B5%AC%EB%8F%85\" aria-label=\"databricks 사용하기구독 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataBricks 사용하기(구독)</h4>\n<p>GCP에서 DataBricks를 사용하기 위해서는 다음과 같이 구독을 먼저 진행해야 합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117228743-a0037880-ae54-11eb-9362-11dd61314007.JPG\" alt=\"da\"></p>\n<ul>\n<li>구독 이후에 DataBricks의 구매 스펙을 정하고 DataBricks Dashborad로 이동하면 됩니다.  </li>\n</ul>\n<br/>\n</li>\n<li>\n<h4 id=\"workspace-생성\" style=\"position:relative;\"><a href=\"#workspace-%EC%83%9D%EC%84%B1\" aria-label=\"workspace 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>WorkSpace 생성</h4>\n<p>Dashborad에서 아래와 같이 사용 할 WorkSpace를 생성하면 됩니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117228897-025c7900-ae55-11eb-941b-597f74ec3f45.JPG\" alt=\"캡처2\"></p>\n<br/>\n</li>\n<li>\n<h4 id=\"workspace-접속\" style=\"position:relative;\"><a href=\"#workspace-%EC%A0%91%EC%86%8D\" aria-label=\"workspace 접속 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>WorkSpace 접속</h4>\n<p>WorkSpace를 생성 후 URL에 접속하면 드디어 데이터 작업을 할 수 있습니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117229354-d42b6900-ae55-11eb-839c-bc1f7979ed4b.JPG\" alt=\"123123123\"></p>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<h4 id=\"databricks-cluster-생성\" style=\"position:relative;\"><a href=\"#databricks-cluster-%EC%83%9D%EC%84%B1\" aria-label=\"databricks cluster 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataBricks Cluster 생성</h4>\n<p>WorkSpace의 Cluster Tab에서 Create Cluster를 클릭해 생성합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117230334-d68ec280-ae57-11eb-939d-295ed700acea.JPG\" alt=\"33333333333333\"></p>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<h4 id=\"저는-다음과-같은-spec으로-cluster를-생성했습니다\" style=\"position:relative;\"><a href=\"#%EC%A0%80%EB%8A%94-%EB%8B%A4%EC%9D%8C%EA%B3%BC-%EA%B0%99%EC%9D%80-spec%EC%9C%BC%EB%A1%9C-cluster%EB%A5%BC-%EC%83%9D%EC%84%B1%ED%96%88%EC%8A%B5%EB%8B%88%EB%8B%A4\" aria-label=\"저는 다음과 같은 spec으로 cluster를 생성했습니다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>저는 다음과 같은 Spec으로 Cluster를 생성했습니다.</h4>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/117380571-f20bd300-af14-11eb-9cae-69720f7c2043.JPG\" alt=\"2222\"></p>\n<ul>\n<li>Nmae : Cluster01 </li>\n<li>Runtime Version : 8.1  </li>\n<li>Worker Type : n2-standard-8 </li>\n<li>Advanced Option Tab을 열어 Google Service Account 입력\n주의 : Service Account는 GCS에 권한이 있어야 합니다.</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>이제 Notebook을 생성하고 GCS를 DBFS에 Mount해서 사용하시면 됩니다.  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">bucket_name <span class=\"token operator\">=</span> <span class=\"token string\">\"nasagcp\"</span>\nmount_name <span class=\"token operator\">=</span> <span class=\"token string\">\"gcpdata\"</span>\ndbutils<span class=\"token punctuation\">.</span>fs<span class=\"token punctuation\">.</span>mount<span class=\"token punctuation\">(</span><span class=\"token string\">\"gs://%s\"</span> <span class=\"token operator\">%</span> bucket_name<span class=\"token punctuation\">,</span> <span class=\"token string\">\"/mnt/%s\"</span> <span class=\"token operator\">%</span> mount_name<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>다음과 같은 형식으로 사용하시면 됩니다.</li>\n</ul>\n</li>\n</ul>\n<br/>\n<ul>\n<li>\n<p>저는 이전에 짜놨었던 스크립트를 다음과 같은 형식으로 사용했습니다. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">bucket_name <span class=\"token operator\">=</span> <span class=\"token string\">\"nasagcp\"</span>\nmount_name <span class=\"token operator\">=</span> <span class=\"token string\">\"gcpdat11\"</span>\ndbutils<span class=\"token punctuation\">.</span>fs<span class=\"token punctuation\">.</span>mount<span class=\"token punctuation\">(</span><span class=\"token string\">\"gs://%s\"</span> <span class=\"token operator\">%</span> bucket_name<span class=\"token punctuation\">,</span> <span class=\"token string\">\"/mnt/%s\"</span> <span class=\"token operator\">%</span> mount_name<span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>context <span class=\"token keyword\">import</span> SparkContext\n<span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>session <span class=\"token keyword\">import</span> SparkSession\n\n\n<span class=\"token comment\"># ------------------------------------------------------------------</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">renameCols</span><span class=\"token punctuation\">(</span>df1<span class=\"token punctuation\">,</span> old_columns<span class=\"token punctuation\">,</span> new_columns<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> old_col<span class=\"token punctuation\">,</span>new_col <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>old_columns<span class=\"token punctuation\">,</span>new_columns<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        d1f <span class=\"token operator\">=</span> df1<span class=\"token punctuation\">.</span>withColumnRenamed<span class=\"token punctuation\">(</span>old_col<span class=\"token punctuation\">,</span>new_col<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> df1\n\n\n<span class=\"token comment\"># Old_columns</span>\nold_columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'avg(min_temperature_air_2m_f)'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'avg(max_temperature_air_2m_f)'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'avg(avg_temperature_air_2m_f)'</span>\n                <span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># New_columns</span>\nnew_columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'temperature_air_min_avg'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'temperature_air_max_avg'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'temperature_air_avg_avg'</span>\n                <span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># --------------------------------------------</span>\n<span class=\"token comment\"># ----------------------</span>\n\n<span class=\"token comment\"># Read CSV from GCS</span>\ndf_lee <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>csv<span class=\"token punctuation\">(</span><span class=\"token string\">\"/mnt/gcpdat11/\"</span><span class=\"token punctuation\">,</span> header<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> inferSchema<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># data transform</span>\ndf_lee <span class=\"token operator\">=</span> df_lee<span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span><span class=\"token string\">'country'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'date'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">'min_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'max_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'avg_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span>desc<span class=\"token punctuation\">(</span><span class=\"token string\">'country'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>orderBy<span class=\"token punctuation\">(</span><span class=\"token string\">'date'</span><span class=\"token punctuation\">)</span>\n\ndf_result <span class=\"token operator\">=</span> renameCols<span class=\"token punctuation\">(</span>df_lee<span class=\"token punctuation\">,</span> old_columns<span class=\"token punctuation\">,</span> new_columns<span class=\"token punctuation\">)</span>\n\ncountry1 <span class=\"token operator\">=</span> df_result<span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span><span class=\"token string\">\"country\"</span><span class=\"token punctuation\">)</span>\ncountry_dis10 <span class=\"token operator\">=</span> df_result<span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span><span class=\"token string\">\"country\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>distinct<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"country_count =\"</span><span class=\"token punctuation\">,</span>country_dis10<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># Write CSV to GCS</span>\ndf_result<span class=\"token punctuation\">.</span>coalesce<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"header\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"true\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mode<span class=\"token punctuation\">(</span><span class=\"token string\">\"overwrite\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>csv<span class=\"token punctuation\">(</span><span class=\"token string\">\"/mnt/gcpdat11/dbfsre/\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>이전에 받아놨던 Covid-19 기상 데이터를 정렬하는 Code 입니다.  </li>\n</ul>\n</li>\n</ul>\n<br/> \n<hr>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며…</h2>\n<p>사실 DataBricks는 사용법에 대한 가이드를 남기기에는 너무 간편합니다..<br>\n그래서 그나마 어려움이 있을 것 같은 DBFS Mount 부분만 설명했습니다.  </p>\n<hr>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#-databricks\">✔ DataBricks?</a></li>\n<li><a href=\"#-gcp-databricks\">👌 GCP DataBricks?</a></li>\n<li><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며…</a></li>\n</ul>\n</div>","excerpt":"머리말   저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다. 이번에는 GCP에서 파트너 SaaS형태로 제공해주는 DataBricks를 사용해서 지난번과 동일한 데이터, 스트립트를 이용해서 성능이나, 사용법에 대한 테스트를 해봤습니다. 물논 이번에도 파이썬을 첨가해서   ✔ DataBricks? og-databricks Databricks란? 간단 요약해서 Spark,Hadoop 등 빅데이터 관련 솔루션 실행환경을 제공하는 클라우드 서비스입니다. 통합 분석 플랫폼으로, 한 WorkSpace내에서 여러 서비스를 사용해 모든 분석이 가능합니다. 이전에 Spark, Hadoop을 ON-Premis…","frontmatter":{"date":"September 12, 2021","title":"[DATA, GCP] - GCP DataBricks 사용기","categories":"GCP DATA","author":"nasa1515","emoji":"🤦‍♂️"},"fields":{"slug":"/data-databricks/"}},"next":{"id":"c0b1f1d9-1ce1-51df-9998-f2fdcca0e5a7","html":"<p>머리말  </p>\n<p>저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다.<br>\n이번에는 DataProc Cluster에 Pyspark Script를 사용해서\n자동화 JOB을 만들어 보겠습니다. 파이썬을 첨가해서  </p>\n<hr>\n<h2 id=\"-data\" style=\"position:relative;\"><a href=\"#-data\" aria-label=\" data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✔ Data</h2>\n<p>Data의 경우에는 이전 포스트에서 다뤘었던 Covid-19의 기상 데이터를 기반으로 진행합니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116961186-9e637480-acdd-11eb-906f-9e340165dee1.JPG\" alt=\"12312312\"></p>\n<ul>\n<li>용량 : 약 51GB</li>\n<li>행 : 542,304,210</li>\n</ul>\n<br/>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116961225-bfc46080-acdd-11eb-930e-ec68574417e5.JPG\" alt=\"2222\"></p>\n<ul>\n<li>데이터 형식 요약</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-python-script\" style=\"position:relative;\"><a href=\"#-python-script\" aria-label=\" python script permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👍 Python Script</h2>\n<p>위의 데이터에서 특정 그룹(나라, 날짜) 별로 MAX,MIN,AVG 값들의 평균 값을 구하는 스크립트 </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>context <span class=\"token keyword\">import</span> SparkContext\n<span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>session <span class=\"token keyword\">import</span> SparkSession\nsc <span class=\"token operator\">=</span> SparkContext<span class=\"token punctuation\">(</span><span class=\"token string\">'local'</span><span class=\"token punctuation\">)</span>\nspark <span class=\"token operator\">=</span> SparkSession<span class=\"token punctuation\">(</span>sc<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">type</span><span class=\"token punctuation\">(</span>spark<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n\nread_path <span class=\"token operator\">=</span> <span class=\"token string\">\"gs://nasa_us/\"</span>\nwrite_path <span class=\"token operator\">=</span> <span class=\"token string\">'gs://proc_result/result/'</span>\n\n\n<span class=\"token comment\"># def for columns cheange</span>\n\n<span class=\"token comment\"># ------------------------------------------------------------------</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">renameCols</span><span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">,</span> old_columns<span class=\"token punctuation\">,</span> new_columns<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> old_col<span class=\"token punctuation\">,</span>new_col <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>old_columns<span class=\"token punctuation\">,</span>new_columns<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        df <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>withColumnRenamed<span class=\"token punctuation\">(</span>old_col<span class=\"token punctuation\">,</span>new_col<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> df\n\n\n<span class=\"token comment\"># Old_columns</span>\nold_columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'avg(min_temperature_air_2m_f)'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'avg(max_temperature_air_2m_f)'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'avg(avg_temperature_air_2m_f)'</span>\n                <span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># New_columns</span>\nnew_columns <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'temperature_air_min_avg'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'temperature_air_max_avg'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'temperature_air_avg_avg'</span>\n                <span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># --------------------------------------------</span>\n<span class=\"token comment\"># ----------------------</span>\n\n<span class=\"token comment\"># Read CSV from GCS</span>\ndf <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>csv<span class=\"token punctuation\">(</span>read_path<span class=\"token punctuation\">,</span> header<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> inferSchema<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># data transform</span>\ndf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span><span class=\"token string\">'country'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'date'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">'min_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'max_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'avg_temperature_air_2m_f'</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'avg'</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\ndf2 <span class=\"token operator\">=</span> renameCols<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">,</span> old_columns<span class=\"token punctuation\">,</span> new_columns<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Write CSV to GCS</span>\ndf2<span class=\"token punctuation\">.</span>coalesce<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"header\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"true\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mode<span class=\"token punctuation\">(</span><span class=\"token string\">\"overwrite\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>csv<span class=\"token punctuation\">(</span>write_path<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>간단 설명 : GCS에서 CSV Format의 Data를 읽고 ETL 작업 후 결과를 GCS에 저장  </li>\n<li>Bigquery Table Data를 csv화 시키고 GCS에 저장하는 방법은 <a href=\"https://nasa1515.tech/gcp_dataproc/\">이전포스트</a>를 확인하세요</li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-dataproc-job-생성\" style=\"position:relative;\"><a href=\"#-dataproc-job-%EC%83%9D%EC%84%B1\" aria-label=\" dataproc job 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👌 DataProc Job 생성</h2>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962299-91945000-ace0-11eb-8e8f-20ea0f9f5b15.JPG\" alt=\"333\"></p>\n<ul>\n<li>위와 같이 DataProc - JOB -> 작업 제출로 JOB을 생성합니다.  </li>\n</ul>\n<br/>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962386-c6a0a280-ace0-11eb-96f5-aaaad00c4588.JPG\" alt=\"44444\"></p>\n<ul>\n<li>Cluster는 실행 할 Cluster를 지정합니다.</li>\n<li>작업 유형은 Pyspark를 선택합니다. </li>\n<li>Python File의 경우 미리 GCS에 올려놓고 지정하면 됩니다.  </li>\n</ul>\n<br/>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962498-12ebe280-ace1-11eb-835a-2b85ed26c91c.JPG\" alt=\"캡처55555\"></p>\n<ul>\n<li>위와 같이 해당 작업이 생성되면서 실행되게 되고<br>\nJOB의 완료 된 후에는 결과 및 로그가 출력되게 됩니다.  </li>\n</ul>\n<br/>\n<hr>\n<h2 id=\"-결과-확인\" style=\"position:relative;\"><a href=\"#-%EA%B2%B0%EA%B3%BC-%ED%99%95%EC%9D%B8\" aria-label=\" 결과 확인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🐱‍🏍 결과 확인</h2>\n<p>Script 실행대로 GCS에 ETL 결과 파일이 다음과 같이 저장되었습니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962590-5c3c3200-ace1-11eb-8614-8e54e3664677.JPG\" alt=\"66666666\"></p>\n<br/>\n<p>그럼 해당 CSV 파일을 기반으로 BigQuery에 Table을 만들어 보겠습니다.  </p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116962680-a0c7cd80-ace1-11eb-84fa-c3e8ca5a092b.JPG\" alt=\"65446565464\"></p>\n<br/>\n<p>데이터를 확인해보면 Script에서 실행 된 ETL 결과만 남아있는 것을 확인 가능합니다.</p>\n<ul>\n<li>\n<p>스키마 데이터</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116963266-59424100-ace3-11eb-9d2a-e3549f04bcae.JPG\" alt=\"77777\"></p>\n<br/>\n</li>\n<li>\n<p>결과 데이터</p>\n<p><img src=\"https://user-images.githubusercontent.com/69498804/116963305-76770f80-ace3-11eb-800d-7cb6f0762ef5.JPG\" alt=\"캡처332131\"></p>\n<br/>\n</li>\n</ul>\n<h2 id=\"끝\" style=\"position:relative;\"><a href=\"#%EB%81%9D\" aria-label=\"끝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>끝!</h2>\n<hr>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며…</h2>\n<p>DataProc에 대해서는 어떻게 사용하는지 대충 알아 본 것 같습니다.<br>\n그래서 다음 포스트에서는 Proc과 동일한 서비스를 제공하는 DataBricks를 사용해보겠습니다.  </p>\n<hr>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#-data\">✔ Data</a></li>\n<li><a href=\"#-python-script\">👍 Python Script</a></li>\n<li><a href=\"#-dataproc-job-%EC%83%9D%EC%84%B1\">👌 DataProc Job 생성</a></li>\n<li><a href=\"#-%EA%B2%B0%EA%B3%BC-%ED%99%95%EC%9D%B8\">🐱‍🏍 결과 확인</a></li>\n<li><a href=\"#%EB%81%9D\">끝!</a></li>\n<li><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며…</a></li>\n</ul>\n</div>","frontmatter":{"date":"September 10, 2021","title":"[DATA, GCP] - GCP DataProc 2탄 Pyspark JOB Access","categories":"GCP DATA","author":"nasa1515","emoji":"🤦‍♂️"},"fields":{"slug":"/gcp-dataproc2/"}},"prev":{"id":"0ad25f11-48a6-55a6-811a-62ec78712b59","html":"<p>머리말  </p>\n<p>저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다.<br>\n이번에는 DataProc Cluster에 Pyspark Script를 사용해서\n자동화 JOB을 만들어 보겠습니다. 파이썬을 첨가해서  </p>\n<hr>\n<h2 id=\"-data\" style=\"position:relative;\"><a href=\"#-data\" aria-label=\" data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>✔ Data</h2>\n<hr>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며…</h2>\n<p>DataProc에 대해서는 어떻게 사용하는지 대충 알아 본 것 같습니다.<br>\n그래서 다음 포스트에서는 Proc과 동일한 서비스를 제공하는 DataBricks를 사용해보겠습니다.  </p>\n<hr>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#-data\">✔ Data</a></li>\n<li><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며…</a></li>\n</ul>\n</div>","frontmatter":{"date":"September 15, 2021","title":"[DATA, AWS] - AWS AppFlow로 Google Analytics Data 수집하기","categories":"AWS DATA","author":"nasa1515","emoji":"🤦‍♂️"},"fields":{"slug":"/data-gadata-appflow/"}},"site":{"siteMetadata":{"siteUrl":"https://nasa1515.com","comments":{"utterances":{"repo":"nasa1515/nasablog"}}}}},"pageContext":{"slug":"/data-databricks/","nextSlug":"/gcp-dataproc2/","prevSlug":"/data-gadata-appflow/"}},"staticQueryHashes":["1073350324","2938748437"]}