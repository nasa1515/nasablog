{"componentChunkName":"component---src-templates-category-template-js","path":"/posts/DATA","result":{"pageContext":{"currentCategory":"DATA","categories":["All","CLOUD","DevOps","LINUX","DATA","Error-Report","NETWORK"],"edges":[{"node":{"id":"8f39d1ff-cc4e-5501-8263-c1575ea2622e","excerpt":"ğŸ˜ About This Post ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Pythonì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•˜ê²Œ ë¬¸ìì—´ì„ ë§Œë“œëŠ” Producerë¥¼ ìƒì„±í•œ ë’¤ì— Kafka-Connectorë¥¼ ì´ìš©í•´ì„œ ê° Cloudì˜ Steraming Tools ë“¤ì„ Endpoint(Broker)ë¡œ Messageë¥¼ ìŒ“ì•„ë³´ê² ìŠµë‹ˆë‹¤.   âœ” Kafka Producer Application KafkaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Clinet APIë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤, ë•Œë¬¸ì— ì´ë¥¼ ì‚¬ìš©í•´ì„œ Producer, Consumerì˜ Application ê°œë°œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. third party Clinetë¥¼ ì‚¬ìš© í•  ìˆ˜ ìˆëŠ”ë° java, python, go ë“±ì´ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”â€¦","fields":{"slug":"/data-kafka-python/"},"frontmatter":{"categories":"DATA CLOUD","title":"[DATA] - Kafka:Confluent to Cloud With Python","date":"May 11, 2022"}},"next":{"fields":{"slug":"/data-kafka-wsl/"}},"previous":null},{"node":{"id":"7ef64a62-2add-5bab-b47b-26a1bed94bf8","excerpt":"ë¨¸ë¦¬ë§   ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Local WSL2 Ubuntuì— Kafka Brockerë¥¼ êµ¬ì„±í•œ ë’¤ ë¬´ì‘ìœ„ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ ê° Cloudì˜ Steraming Tools (aws : kinesis, gcp : pub/sub, azure : eventhub)ì—ì„œ Consumeí•˜ëŠ” ê³¼ì •ì„ ì •ë¦¬í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ì•„ë¬´ë˜ë„ ê¸ˆì•¡ì ì¸ ë¶€ë¶„ì˜ ì´ìŠˆê°€ ë°œìƒí•˜ê¸°ì— ìµœëŒ€í•œ egress Trafficì´ ë°œìƒë˜ì§€ ì•Šê²Œ Localì—ì„œ ì§„í–‰í•˜ê²Œë˜ì—ˆìŠµë‹ˆë‹¤.    âœ” Docker, Docker-compose ì„¤ì¹˜ Dockerì™€ Docker-composeì˜ ê²½ìš° ì•„ë˜ì˜ ê³µì‹ë¬¸ì„œë¥¼ í™•ì¸í•˜ì‹œë©´ ìì„¸í•œ ì„¤ì¹˜ ë°©ë²•ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Docker ì„¤ì¹˜ Dâ€¦","fields":{"slug":"/data-kafka-wsl/"},"frontmatter":{"categories":"DATA CLOUD","title":"[DATA] - Zookeeper & Kafka êµ¬ì„± with WSL2, Docker","date":"May 09, 2022"}},"next":{"fields":{"slug":"/azure-oracle19/"}},"previous":{"fields":{"slug":"/data-kafka-python/"}}},{"node":{"id":"ae20c5ad-ffd3-5801-9cbf-a67512c9d035","excerpt":"ë¨¸ë¦¬ë§   ì´ë²ˆ í¬ìŠ¤íŠ¸ë„ ì—­ì‹œ íŒŒì´ì¬ì„ ì²¨ê°€í–ˆìŠµë‹ˆë‹¤. MicroSoftì—ì„œ ì œê³µí•˜ëŠ” BotFrameworkì„ ì‚¬ìš©í•´ì„œ ê°„ë‹¨í•œ ì§ˆë‹µì„ í•˜ëŠ” ChatBotì„ ìƒì„±í•œ ë’¤ Azure Webì— ë°°í¬í•˜ê³  Teams Appì— ì—°ë™ í•´ë³´ê² ìŠµë‹ˆë‹¤.   âœ” BotFrameWork MicroSoftì—ì„œ ì œê³µí•˜ê³  ìˆëŠ” Chatbot SDK OpenSource ì…ë‹ˆë‹¤. C#, JS, Python, Java ë“± ì—¬ëŸ¬ ì–¸ì–´ë¥¼ ì‚¬ìš©í•´ì„œ SDKë¥¼ ì‚¬ìš© í•  ìˆ˜ ìˆê³  ì œì‘í•œ í…œí”Œë¦¿ì„ ì‰½ê²Œ Azureì˜ Serviceì™€ ì—°ë™ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   GITHUB   âœŒ 1. Bot ìƒì„± ë°”ë¡œ Bot ìƒì„±ì— ì•ì„œ ì§„í–‰ ì „ ì„ í–‰ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼í•©ë‹ˆë‹¤.  ğŸ‘ Fraâ€¦","fields":{"slug":"/azure-chatbot/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, AZURE] - MicroSoft BotFrameWork with Python to Azure","date":"October 24, 2021"}},"next":{"fields":{"slug":"/data-databricks/"}},"previous":{"fields":{"slug":"/azure-oracle19/"}}},{"node":{"id":"14668db0-2f50-511a-abf9-9d92fcdc88bd","excerpt":"ë¨¸ë¦¬ë§   ì €ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ DataProcì— ëŒ€í•œ ì„¤ëª…ê³¼ ê°„ë‹¨í•œ ì‚¬ìš©ë²•ì„ ë‹¤ë¤„ë´¤ì—ˆìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” GCPì—ì„œ íŒŒíŠ¸ë„ˆ SaaSí˜•íƒœë¡œ ì œê³µí•´ì£¼ëŠ” DataBricksë¥¼ ì‚¬ìš©í•´ì„œ ì§€ë‚œë²ˆê³¼ ë™ì¼í•œ ë°ì´í„°, ìŠ¤íŠ¸ë¦½íŠ¸ë¥¼ ì´ìš©í•´ì„œ ì„±ëŠ¥ì´ë‚˜, ì‚¬ìš©ë²•ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë´¤ìŠµë‹ˆë‹¤. ë¬¼ë…¼ ì´ë²ˆì—ë„ íŒŒì´ì¬ì„ ì²¨ê°€í•´ì„œ   âœ” DataBricks? og-databricks Databricksë€? ê°„ë‹¨ ìš”ì•½í•´ì„œ Spark,Hadoop ë“± ë¹…ë°ì´í„° ê´€ë ¨ ì†”ë£¨ì…˜ ì‹¤í–‰í™˜ê²½ì„ ì œê³µí•˜ëŠ” í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. í†µí•© ë¶„ì„ í”Œë«í¼ìœ¼ë¡œ, í•œ WorkSpaceë‚´ì—ì„œ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•´ ëª¨ë“  ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ì „ì— Spark, Hadoopì„ ON-Premisâ€¦","fields":{"slug":"/data-databricks/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, GCP] - GCP DataBricks ì‚¬ìš©ê¸°","date":"September 12, 2021"}},"next":{"fields":{"slug":"/gcp-dataproc2/"}},"previous":{"fields":{"slug":"/azure-chatbot/"}}},{"node":{"id":"c0b1f1d9-1ce1-51df-9998-f2fdcca0e5a7","excerpt":"ë¨¸ë¦¬ë§   ì €ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ DataProcì— ëŒ€í•œ ì„¤ëª…ê³¼ ê°„ë‹¨í•œ ì‚¬ìš©ë²•ì„ ë‹¤ë¤„ë´¤ì—ˆìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” DataProc Clusterì— Pyspark Scriptë¥¼ ì‚¬ìš©í•´ì„œ \nìë™í™” JOBì„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤. íŒŒì´ì¬ì„ ì²¨ê°€í•´ì„œ   âœ” Data Dataì˜ ê²½ìš°ì—ëŠ” ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ë¤˜ì—ˆë˜ Covid-19ì˜ ê¸°ìƒ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.   12312312 ìš©ëŸ‰ : ì•½ 51GB í–‰ : 542,304,210 2222 ë°ì´í„° í˜•ì‹ ìš”ì•½ ğŸ‘ Python Script ìœ„ì˜ ë°ì´í„°ì—ì„œ íŠ¹ì • ê·¸ë£¹(ë‚˜ë¼, ë‚ ì§œ) ë³„ë¡œ MAX,MIN,AVG ê°’ë“¤ì˜ í‰ê·  ê°’ì„ êµ¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸  ê°„ë‹¨ ì„¤ëª… : GCSì—ì„œ CSV Formatì˜ Dataë¥¼ â€¦","fields":{"slug":"/gcp-dataproc2/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, GCP] - GCP DataProc 2íƒ„ Pyspark JOB Access","date":"September 10, 2021"}},"next":{"fields":{"slug":"/gcp-dataproc/"}},"previous":{"fields":{"slug":"/data-databricks/"}}},{"node":{"id":"f1eefdc3-7c74-544e-b4c9-728dcf1c72c7","excerpt":"ë¨¸ë¦¬ë§   ì´ë²ˆì—ëŠ” DataProc(Hadoop/Spark)ë¥¼ ì‚¬ìš©í•˜ì—¬ \nëŒ€ìš©ëŸ‰ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ë‹¤ë£¹ë‹ˆë‹¤. ë¬¼ë¡  íŒŒì´ì¬ì„ ì²¨ê°€í•´ì„œ   âœ” DataProcì— ëŒ€í•´ì„œ.. Dataprocì€ ì¼ê´„ ì²˜ë¦¬, ì¿¼ë¦¬, ìŠ¤íŠ¸ë¦¬ë°, ë¨¸ì‹  ëŸ¬ë‹ì— ì˜¤í”ˆì†ŒìŠ¤ ë°ì´í„° ë„êµ¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ê´€ë¦¬í˜• Spark ë° Hadoop ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì¦‰ ì§€ê¸ˆê¹Œì§€ ê·€ì°®ê²Œ Spark, Hadoopì„ ì—°ë™í•˜ëŠ” ê³¼ì •ì„ ì—†ì• ê³  ì‚¬ìš©ë§Œí•˜ë©´ ë˜ëŠ” ì„œë¹„ìŠ¤ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   ì—¬ê¸°ì„œ DataFlowì™€ DataProcì˜ ì°¨ì´ì— ëŒ€í•´ì„œ ê¶ê¸ˆì¦ì´ ìƒê²¼ëŠ”ë° ë‘ íˆ´ ëª¨ë‘ ETLì„ í•˜ëŠ” íˆ´ì— ëŒ€í•´ì„œëŠ” ê³µí†µì ì„ ê°€ì§€ê³  ìˆì§€ë§Œ DataFlowëŠ” Serverlesâ€¦","fields":{"slug":"/gcp-dataproc/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, GCP] - GCP DataProc spark Clusterë¡œ ETL í›„ BigQueryì— ì ì¬","date":"September 08, 2021"}},"next":{"fields":{"slug":"/azure-datafactory/"}},"previous":{"fields":{"slug":"/gcp-dataproc2/"}}},{"node":{"id":"899a2ae9-4b12-5939-9578-b5831fff8cb9","excerpt":"ë¨¸ë¦¬ë§   ìš” ê·¼ë˜ ë¸”ë¡œê·¸ Rebuild, ì—…ë¬´ ë“±ë“±ë“±â€¦ë„ˆë¬´ ë°”ìœ í•˜ë£¨ì˜€ìŠµë‹ˆë‹¤. (ğŸ¤¦â€â™‚ï¸ ì•„ì§ë„ ë°”ì˜ê¸´ í•˜ì§€ë§Œ;;) ê·¸ë˜ë„ ì£¼ë§, í‡´ê·¼ ì´í›„ì— ê¸°ìˆ ê³µë¶€ í•˜ëŠ” ì‹œê°„ ì¤‘ì— ìª¼ë”ì´ë‚˜ë§ˆ ì§¬ì„ë‚´ ë¸”ë¡œê·¸ ì—…ë°ì´íŠ¸ë¥¼ í•˜ë ¤ê³  ë…¸ë ¥ì¤‘ì…ë‹ˆë‹¤!! ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Azureì˜ DataFactoryì˜ ì´ë¡ ì ì¸ ë‚´ìš©ê³¼ ì‹¤ì œ Oracle DBì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤. âœ” Azure DataFactory? image\në°ì´í„° ì´ë™ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•˜ê³  ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” ë°ì´í„° ì›Œí¬í”Œë¡œë¥¼ ë§Œë“¤ ìˆ˜ ìˆëŠ” í´ë¼ìš°ë“œ ê¸°ë°˜ ETL ë° ë°ì´í„° í†µí•© ì„œë¹„ìŠ¤ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì €ì¥ì†Œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ê¸°ë°˜ ì›Œí¬í”Œë¡œ(íŒŒì´í”„ë¼ì¸ì´ë¼ê³  í•¨)ë¥¼ ë§Œë“¤â€¦","fields":{"slug":"/azure-datafactory/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, AZURE] Azure DataFactoryë¡œ Oracle Data ìˆ˜ì§‘í•˜ê¸°","date":"September 05, 2021"}},"next":{"fields":{"slug":"/data-gcpdataflow/"}},"previous":{"fields":{"slug":"/gcp-dataproc/"}}},{"node":{"id":"d7e3e240-ef5e-5891-af05-6c06162c25f1","excerpt":"ë¨¸ë¦¬ë§   ìš”ì¦˜ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„± í•  ì‹œê°„ì´ ë¶€ì¡±í–ˆìŠµë‹ˆë‹¤â€¦(ì¼â€¦) ê·¸ë˜ì„œ ì˜¤ëœë§Œì— í¬ìŠ¤íŠ¸ë¥¼ ì˜¬ë¦° ê¸°ë…ìœ¼ë¡œ ì´ë²ˆ ë‚´ìš©ì„ ë”ìš± ì•Œì°¨ê²Œ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” GCPì˜ DataFlowë¥¼ ì‚¬ìš©í•´ GCSì— ìˆëŠ” CSV íŒŒì¼ì„ ê°„ë‹¨í•œ Parsing ì‘ì—…ì„ í•œ ë’¤ BigQuery Tableì— ì ì¬í•˜ëŠ” ë¶€ë¶„ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. ë¬¼ë¡  íŒŒì´ì¬ì„ ì²¨ê°€í•´ì„œ   âœ” DataFlowì— ëŒ€í•´ì„œ.. DataFlowëŠ” GCPì—ì„œ DataPipeline(ETL, MR ë“±)ì„ Apache Beam ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ë„ë¡ ë§Œë“  Runtime Service ì…ë‹ˆë‹¤. ìŒ ê°„ë‹¨í•˜ê²Œ ë§í•˜ë©´ Spark Stremingì´ë‚˜ Batch ì²˜ë¦¬ë¥¼ Cloudë¥¼ ì‚¬ìš©í•´ Paaâ€¦","fields":{"slug":"/data-gcpdataflow/"},"frontmatter":{"categories":"DATA CLOUD","title":"[DATA] - GCP DataFlow, csv from GCS to BigQuery With Python","date":"September 02, 2021"}},"next":{"fields":{"slug":"/azure-coludshellerror/"}},"previous":{"fields":{"slug":"/azure-datafactory/"}}},{"node":{"id":"2113ff4f-0a02-5bae-b5c9-faf24641bc18","excerpt":"ë¨¸ë¦¬ë§   ì´ì „ì— í•œë²ˆ Standalone Clusterë¡œ Sparkë¥¼ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Azureì™€ ì—°ë™í•˜ëŠ” ê³¼ì •ì—ì„œ ì—¬ëŸ¬ê°€ì§€ ë¬¸ì œê°€ ë°œìƒí–ˆê³  ê²°êµ­ ì´ì „ í¬ìŠ¤íŠ¸ì¸ Hadoop Clusterë¥¼ êµ¬ì„±í•´ì„œ Sparkë¥¼ êµ¬ë™ì‹œí‚¤ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì„¤ì¹˜í•œ Hadoop Clusterì˜ yarnì— Sparkë¥¼ êµ¬ë™ì‹œí‚¤ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.   âœ” Spark ì„¤ì¹˜ JDK ë“±ì˜ ê¸°ë³¸ì ì¸ í™˜ê²½ì„¤ì •ì€ ì´ì „í¬ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Sparkì˜ ì„¤ì¹˜ë³´ë‹¤ëŠ” yarnê³¼ì˜ ì—°ë™ë¶€ë¶„ì„ ì¤‘ì ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.   Python ì„¤ì¹˜ (pysparkë¥¼ ìœ„í•¨) Spark ê³„ì • ì„¤ì • Spark ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜ Spaâ€¦","fields":{"slug":"/data-sparkonyarn/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Apache Spark v3.0 on yarn ì„¤ì¹˜ With Zeppelin","date":"August 16, 2021"}},"next":{"fields":{"slug":"/data-hadoopinstall/"}},"previous":{"fields":{"slug":"/azure-aksconnect/"}}},{"node":{"id":"cc3f2da4-d9dd-5454-a99e-eba728ad8ea2","excerpt":"ë¨¸ë¦¬ë§   ì•ì´ ë§‰ë§‰í•©ë‹ˆë‹¤. ì €ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ ì¸í”„ë¼ êµ¬ì„±ì„ ëëƒˆì–´ì•¼ í–ˆëŠ”ë°â€¦ ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œë¼ë„ ë§ˆë¬´ë¦¬ ì§€ì–´ë³´ì£    âœ” ì„¤ì¹˜ í™˜ê²½ Hadoop 3.3.0 (Full-Distribute Mode) Server Master Worker1 Worker2 OS CentOS 8.2 CentOS 8.2 CentOS 8.2 Disk 30G 30G 30G MEM 14G 14G 14G CPU 4.Core 4.Core 4.Core VM (Azure) Hadoop Master IP : 10.0.0.5  Hadoop Worker1 IP : 10.0.0.6  Hadoop Worker2 IP : 10.0.0.7  âœŒ Hadoop ì„¤ì¹˜ ì „â€¦","fields":{"slug":"/data-hadoopinstall/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Hadoop 3.3.0 Full Distribute mode infra êµ¬ì¶•","date":"August 15, 2021"}},"next":{"fields":{"slug":"/data-sparkinstall/"}},"previous":{"fields":{"slug":"/data-sparkonyarn/"}}},{"node":{"id":"55f2967c-b271-5121-9dc5-8bebc2e1c7a9","excerpt":"ë¨¸ë¦¬ë§   ì €ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ Apache Sparkê°€ ì–´ë–¤ ì‹ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€? ì–´ë–¤ í•¨ìˆ˜ê°€ ìˆëŠ”ì§€? ê°„ë‹¨í•˜ê²Œ ì´ë¡ ì ìœ¼ë¡œë§Œ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤. ì•„ì§ Sparkì— ëŒ€í•œ ë‚´ìš©ì´ ì œëŒ€ë¡œ ì´í•´ê°€ ë˜ì§€ ì•Šì•„ ì¼ë‹¨ êµ¬ì„±ë¶€í„° í•´ë³´ê³  ì‹¤ìŠµì„ í•˜ë©´ì„œ ë‹¤ì‹œ ì´í•´ë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤.   âœ” Azure VMì— Spark StandAlone êµ¬ì„± Spark StandAlone Clusterë¡œ êµ¬ì„±í•˜ëŠ” í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.  í™˜ê²½êµ¬ì„± OS : CentOS Linux release 8.2.2004 (Core)   cpu : 4 core   RAM : 14GB   JDK : 1.8.0 python : 3.8.8 Spark 3.0.2 zeppelin : 0.9.0 1â€¦","fields":{"slug":"/data-sparkinstall/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Azure VMì— Apache Spark v3.0 Standalone ì„¤ì¹˜ With Zeppelin","date":"August 14, 2021"}},"next":{"fields":{"slug":"/data-hadoopeco/"}},"previous":{"fields":{"slug":"/data-hadoopinstall/"}}},{"node":{"id":"3b573dbf-a917-56a5-9a76-6736d98f7ec3","excerpt":"ë¨¸ë¦¬ë§   ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œ Hadoop EcoSystem ì¤‘ Core Projectì— ëŒ€í•´ì„œ ë‹¤ë¤˜ì—ˆìŠµë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ DBí™” í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ë“¤ì˜ ëª¨ìŒì¸ SUB Projectë“¤ì— ëŒ€í•´ì„œ ë‹¤ë£¹ë‹ˆë‹¤. ëª¨ë“  í”„ë¡œì íŠ¸ë¥¼ ë‹¤ë£¨ì§€ëŠ” ì•Šê³  ì•ìœ¼ë¡œ ì‚¬ìš©í•˜ê²Œ ë  ê²ƒ ê°™ì€ í”„ë¡œì íŠ¸ ìœ„ì£¼ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.     âœ” Hadoop EcoSystem Sub Project 123123123 ì´ì „í¬ìŠ¤íŠ¸ì—ì„œëŠ” Hadoop EcoSystemì˜ Core Project ë¶€ë¶„ì— ëŒ€í•´ì„œ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. Core ProjectëŠ” ë‹¤ ì„¤ëª…í–ˆê³  ì´ì œ Hadoop Sub Projectì˜ ì°¨ë¡€ ì…ë‹ˆë‹¤.  Hadoop Core Project : Hâ€¦","fields":{"slug":"/data-hadoopeco/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Hadoop EcoSystem Sub Project","date":"August 13, 2021"}},"next":{"fields":{"slug":"/data-hadoop/"}},"previous":{"fields":{"slug":"/data-sparkinstall/"}}},{"node":{"id":"e82d4a67-91b1-5218-9fc2-35c7377e5ffc","excerpt":"ë¨¸ë¦¬ë§   ì´ë²ˆ ë‚´ìš©ì€ ì´ì „ì— Sparkì˜ ì´ë¡ ì ì¸ ì„¤ëª…ì„ ì´ì–´ì„œ ë” ëŒ€í‘œì ì¸ Hadoopì— ëŒ€í•´ì„œ ì´ë¡ ì ì¸ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•´ë³´ëŠ” í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì €ëŠ” ì—¬ëŸ¬ í¬ìŠ¤íŠ¸ë¡œ ì‹¤ì œ Clusterë¥¼ êµ¬ì¶•í•˜ê¸´ í–ˆì§€ë§Œ HDFSê°€ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì €ì¥í•˜ëŠ”ì§€, ecosystemì´ ë­ì§€? ë¼ëŠ” ì˜ë¬¸ì´ ë§ì´ ë‚¨ì•˜ê¸°ì— ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•  í•„ìš”ë¥¼ ëŠê¼ˆìŠµë‹ˆë‹¤.   âœ” Apache Hadoop? 1111123123 Hadoop : í•˜ë‘¡ ì†Œí”„íŠ¸ì›¨ì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ê°„ë‹¨í•œ í”„ë¡œê·¸ë˜ë° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ëŒ€ì˜ ì»´í“¨í„° í´ëŸ¬ìŠ¤í„°ì—ì„œ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¶„ì‚° ì²˜ë¦¬ í•  ìˆ˜ìˆê²Œ í•´ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ ì´ë‹¤. ë¼ê³  ëª¨ë“  ê¸€ì—ì„œ ì„¤ëª…ì„ í•˜ëŠ”ë° ë‚˜ëŠ” ê·¸ëƒ¥ ë°ì´í„°ë¥¼ ë¶„ì‚° ì €â€¦","fields":{"slug":"/data-hadoop/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Apache Hadoop, HDFS, MapReduce","date":"August 13, 2021"}},"next":{"fields":{"slug":"/date-spark/"}},"previous":{"fields":{"slug":"/data-hadoopeco/"}}},{"node":{"id":"2a5bcac3-907a-58f0-81da-8f92654bc3e6","excerpt":"ë¨¸ë¦¬ë§   ì´ë²ˆì—ëŠ” ë°ì´í„°ì˜ ê°€ì¥ ê¸°ì´ˆì ì¸ ì˜¤í”ˆì†ŒìŠ¤ì¸ Apache Sparkì— ëŒ€í•œ ë‚´ìš© ì •ë¦¬ì…ë‹ˆë‹¤. ì•„ë¬´ê²ƒë„ ëª¨ë¥´ëŠ” ìƒì§œ ì´ˆë³´ì´ê¸° ë•Œë¬¸ì— í‹€ë¦° ë¶€ë¶„ì´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   âœ” Apache Spark? Hadoop? ìº¡ì²˜1 ì£¼ì›Œë“¤ì€ ë§ë¡œëŠ” ë°ì´í„° ì‹œì¥ì€ ì˜¤í”ˆì†ŒìŠ¤ì¸ Hadoopê³¼ Apacheê°€ ê²½ìŸí•˜ë©° ì„±ì¥í•˜ê³  ìˆë‹¤ê³  ì•Œê³  ìˆë‹¤ ê·¸ëŸ°ë° ë˜ ë‹¤ë¥¸ ê¸€ë“¤ì„ ë³´ë‹ˆ ì´ë¯¸ ì—…ê³„ì—ì„œëŠ” ë‘ ì˜¤í”ˆì†ŒìŠ¤ë¥¼ ë™ì‹œì— ì‚¬ìš©í•œë‹¤ê³ ë„ í•œë‹¤. ê²½ìŸí•˜ëŠ” ê´€ê³„ì¸ë° ë˜ ìƒìƒì„ í•˜ê³  ìˆë‹¤ëŠ”ê²Œ ë¬´ìŠ¨ì†Œë¦¬ì§€? ë‹¤ì‹œ í•œë²ˆ ì°¾ì•„ë³´ë‹ˆ ê°ê°ì˜ íˆ´ì˜ ìš©ë„ì— ëŒ€í•´ì„œ ì•Œì§€ ëª»í–ˆë˜ ë‚˜ì˜ ì˜¤ì°©ì´ì—ˆë‹¤.   ë‚´ê°€ ì´í•´í•œ ë‘ ì•±ì˜ ìš©ë„ë¥¼ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ë³´ë©´ ìš°ì„  ë‘ íˆ´ì€ ë¹…ë°ì´í„° â€¦","fields":{"slug":"/date-spark/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Apache Sparkë€??","date":"August 13, 2021"}},"next":{"fields":{"slug":"/devops-sonarqube/"}},"previous":{"fields":{"slug":"/data-hadoop/"}}},{"node":{"id":"887be352-bf3c-5019-b2eb-f2dcb2cfba4d","excerpt":"ë¨¸ë¦¬ë§   ë¸”ë¡œê·¸ì—ë„ ë§¤ë²ˆ ì¸í”„ë¼ë‚˜ Devops ê´€ë ¨ ê¸€ë“¤ë§Œ ì˜¬ë¼ì™€ì„œ ìµœê·¼ì— ê³µë¶€í•˜ê³  ìˆëŠ” Dataìª½ë„ í¬ìŠ¤íŠ¸ë¥¼ ëŠ˜ë¦¬ë ¤ê³  í•©ë‹ˆë‹¤. ì•„ì§ ì´ˆê¸‰ì ìˆ˜ì¤€ì´ë¼ì„œ í‹€ë¦° ë‚´ìš©ì´ ë§ì„ ê²ƒ ê°™ì§€ë§Œ, ë³µìŠµí•˜ëŠ” ëŠë‚Œìœ¼ë¡œâ€¦ ë³¸ í¬ìŠ¤íŠ¸ì—ì„œ ë‚´ìš©ë“¤ì€ ëª¨ë“œ MS Docë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë¦¬í•´ ì‘ì„±í–ˆìŠµë‹ˆë‹¤. âœ” Azure Synapse Analytics Synapse AnalyticsëŠ” ì—”í„°í”„ë¼ì´ì¦ˆ ë°ì´í„° ì›¨ì–´í•˜ìš°ì§•ê³¼ ë¹… ë°ì´í„° ë¶„ì„ì„ ê²°í•©í•œ SaaS ì…ë‹ˆë‹¤. Synapseì˜ ìš©ì–´ ì¤‘ì˜ SQL Pool (SQL DW)ì´ë€?? Synapse Analyticsì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  Enter Prise Data WareHousing ì—”í„°â€¦","fields":{"slug":"/azure-synapse/"},"frontmatter":{"categories":"CLOUD DATA","title":"[AZURE] [DATA] Azure Synapse Analytics","date":"August 02, 2021"}},"next":{"fields":{"slug":"/azure-web/"}},"previous":{"fields":{"slug":"/devops-cicd1/"}}}]}},"staticQueryHashes":["1073350324","2938748437"]}