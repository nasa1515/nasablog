{"componentChunkName":"component---src-templates-category-template-js","path":"/posts/DATA","result":{"pageContext":{"currentCategory":"DATA","categories":["All","CLOUD","DevOps","LINUX","DATA","Error-Report","NETWORK"],"edges":[{"node":{"id":"8f39d1ff-cc4e-5501-8263-c1575ea2622e","excerpt":"😎 About This Post 이번 포스트에서는 Python을 사용하여 간단하게 문자열을 만드는 Producer를 생성한 뒤에 Kafka-Connector를 이용해서 각 Cloud의 Steraming Tools 들을 Endpoint(Broker)로 Message를 쌓아보겠습니다.   ✔ Kafka Producer Application Kafka는 기본적으로 Clinet API를 가지고 있습니다, 때문에 이를 사용해서 Producer, Consumer의 Application 개발이 가능합니다. third party Clinet를 사용 할 수 있는데 java, python, go 등이 있습니다. 가장 대표적으로 사용하는…","fields":{"slug":"/data-kafka-python/"},"frontmatter":{"categories":"DATA CLOUD","title":"[DATA] - Kafka:Confluent to Cloud With Python","date":"May 11, 2022"}},"next":{"fields":{"slug":"/data-kafka-wsl/"}},"previous":null},{"node":{"id":"7ef64a62-2add-5bab-b47b-26a1bed94bf8","excerpt":"머리말   이번 포스트에서는 Local WSL2 Ubuntu에 Kafka Brocker를 구성한 뒤 무작위 데이터를 생성하여 각 Cloud의 Steraming Tools (aws : kinesis, gcp : pub/sub, azure : eventhub)에서 Consume하는 과정을 정리해보았습니다. 아무래도 금액적인 부분의 이슈가 발생하기에 최대한 egress Traffic이 발생되지 않게 Local에서 진행하게되었습니다.    ✔ Docker, Docker-compose 설치 Docker와 Docker-compose의 경우 아래의 공식문서를 확인하시면 자세한 설치 방법을 확인할 수 있습니다. Docker 설치 D…","fields":{"slug":"/data-kafka-wsl/"},"frontmatter":{"categories":"DATA CLOUD","title":"[DATA] - Zookeeper & Kafka 구성 with WSL2, Docker","date":"May 09, 2022"}},"next":{"fields":{"slug":"/azure-oracle19/"}},"previous":{"fields":{"slug":"/data-kafka-python/"}}},{"node":{"id":"ae20c5ad-ffd3-5801-9cbf-a67512c9d035","excerpt":"머리말   이번 포스트도 역시 파이썬을 첨가했습니다. MicroSoft에서 제공하는 BotFramework을 사용해서 간단한 질답을 하는 ChatBot을 생성한 뒤 Azure Web에 배포하고 Teams App에 연동 해보겠습니다.   ✔ BotFrameWork MicroSoft에서 제공하고 있는 Chatbot SDK OpenSource 입니다. C#, JS, Python, Java 등 여러 언어를 사용해서 SDK를 사용 할 수 있고 제작한 템플릿을 쉽게 Azure의 Service와 연동 할 수 있습니다.   GITHUB   ✌ 1. Bot 생성 바로 Bot 생성에 앞서 진행 전 선행조건을 만족해야합니다.  👍 Fra…","fields":{"slug":"/azure-chatbot/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, AZURE] - MicroSoft BotFrameWork with Python to Azure","date":"October 24, 2021"}},"next":{"fields":{"slug":"/data-databricks/"}},"previous":{"fields":{"slug":"/azure-oracle19/"}}},{"node":{"id":"14668db0-2f50-511a-abf9-9d92fcdc88bd","excerpt":"머리말   저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다. 이번에는 GCP에서 파트너 SaaS형태로 제공해주는 DataBricks를 사용해서 지난번과 동일한 데이터, 스트립트를 이용해서 성능이나, 사용법에 대한 테스트를 해봤습니다. 물논 이번에도 파이썬을 첨가해서   ✔ DataBricks? og-databricks Databricks란? 간단 요약해서 Spark,Hadoop 등 빅데이터 관련 솔루션 실행환경을 제공하는 클라우드 서비스입니다. 통합 분석 플랫폼으로, 한 WorkSpace내에서 여러 서비스를 사용해 모든 분석이 가능합니다. 이전에 Spark, Hadoop을 ON-Premis…","fields":{"slug":"/data-databricks/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, GCP] - GCP DataBricks 사용기","date":"September 12, 2021"}},"next":{"fields":{"slug":"/gcp-dataproc2/"}},"previous":{"fields":{"slug":"/azure-chatbot/"}}},{"node":{"id":"c0b1f1d9-1ce1-51df-9998-f2fdcca0e5a7","excerpt":"머리말   저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다. 이번에는 DataProc Cluster에 Pyspark Script를 사용해서 \n자동화 JOB을 만들어 보겠습니다. 파이썬을 첨가해서   ✔ Data Data의 경우에는 이전 포스트에서 다뤘었던 Covid-19의 기상 데이터를 기반으로 진행합니다.   12312312 용량 : 약 51GB 행 : 542,304,210 2222 데이터 형식 요약 👍 Python Script 위의 데이터에서 특정 그룹(나라, 날짜) 별로 MAX,MIN,AVG 값들의 평균 값을 구하는 스크립트  간단 설명 : GCS에서 CSV Format의 Data를 …","fields":{"slug":"/gcp-dataproc2/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, GCP] - GCP DataProc 2탄 Pyspark JOB Access","date":"September 10, 2021"}},"next":{"fields":{"slug":"/gcp-dataproc/"}},"previous":{"fields":{"slug":"/data-databricks/"}}},{"node":{"id":"f1eefdc3-7c74-544e-b4c9-728dcf1c72c7","excerpt":"머리말   이번에는 DataProc(Hadoop/Spark)를 사용하여 \n대용량의 데이터를 처리하는 방법에 대해서 다룹니다. 물론 파이썬을 첨가해서   ✔ DataProc에 대해서.. Dataproc은 일괄 처리, 쿼리, 스트리밍, 머신 러닝에 오픈소스 데이터 도구를 활용할 수 있는 관리형 Spark 및 Hadoop 서비스입니다. 즉 지금까지 귀찮게 Spark, Hadoop을 연동하는 과정을 없애고 사용만하면 되는 서비스라고 볼 수 있습니다.   여기서 DataFlow와 DataProc의 차이에 대해서 궁금증이 생겼는데 두 툴 모두 ETL을 하는 툴에 대해서는 공통점을 가지고 있지만 DataFlow는 Serverles…","fields":{"slug":"/gcp-dataproc/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, GCP] - GCP DataProc spark Cluster로 ETL 후 BigQuery에 적재","date":"September 08, 2021"}},"next":{"fields":{"slug":"/azure-datafactory/"}},"previous":{"fields":{"slug":"/gcp-dataproc2/"}}},{"node":{"id":"899a2ae9-4b12-5939-9578-b5831fff8cb9","excerpt":"머리말   요 근래 블로그 Rebuild, 업무 등등등…너무 바쁜 하루였습니다. (🤦‍♂️ 아직도 바쁘긴 하지만;;) 그래도 주말, 퇴근 이후에 기술공부 하는 시간 중에 쪼끔이나마 짬을내 블로그 업데이트를 하려고 노력중입니다!! 이번 포스트에서는 Azure의 DataFactory의 이론적인 내용과 실제 Oracle DB의 데이터를 수집하는 내용입니다. ✔ Azure DataFactory? image\n데이터 이동을 오케스트레이션하고 데이터를 변환하는 데이터 워크플로를 만들 수 있는 클라우드 기반 ETL 및 데이터 통합 서비스 서로 다른 데이터 저장소의 데이터를 수집할 수 있는 데이터 기반 워크플로(파이프라인이라고 함)를 만들…","fields":{"slug":"/azure-datafactory/"},"frontmatter":{"categories":"CLOUD DATA","title":"[DATA, AZURE] Azure DataFactory로 Oracle Data 수집하기","date":"September 05, 2021"}},"next":{"fields":{"slug":"/data-gcpdataflow/"}},"previous":{"fields":{"slug":"/gcp-dataproc/"}}},{"node":{"id":"d7e3e240-ef5e-5891-af05-6c06162c25f1","excerpt":"머리말   요즘 포스트를 작성 할 시간이 부족했습니다…(일…) 그래서 오랜만에 포스트를 올린 기념으로 이번 내용을 더욱 알차게 준비했습니다. 이번 포스트에서는 GCP의 DataFlow를 사용해 GCS에 있는 CSV 파일을 간단한 Parsing 작업을 한 뒤 BigQuery Table에 적재하는 부분을 다뤘습니다. 물론 파이썬을 첨가해서   ✔ DataFlow에 대해서.. DataFlow는 GCP에서 DataPipeline(ETL, MR 등)을 Apache Beam 기반으로 동작하도록 만든 Runtime Service 입니다. 음 간단하게 말하면 Spark Streming이나 Batch 처리를 Cloud를 사용해 Paa…","fields":{"slug":"/data-gcpdataflow/"},"frontmatter":{"categories":"DATA CLOUD","title":"[DATA] - GCP DataFlow, csv from GCS to BigQuery With Python","date":"September 02, 2021"}},"next":{"fields":{"slug":"/azure-coludshellerror/"}},"previous":{"fields":{"slug":"/azure-datafactory/"}}},{"node":{"id":"2113ff4f-0a02-5bae-b5c9-faf24641bc18","excerpt":"머리말   이전에 한번 Standalone Cluster로 Spark를 설치하는 방법을 알아봤습니다. 그러나 Azure와 연동하는 과정에서 여러가지 문제가 발생했고 결국 이전 포스트인 Hadoop Cluster를 구성해서 Spark를 구동시키기로 했습니다. 이번 포스트에서는 설치한 Hadoop Cluster의 yarn에 Spark를 구동시키는 과정입니다.   ✔ Spark 설치 JDK 등의 기본적인 환경설정은 이전포스트를 확인해주세요. 이번 포스트에서는 Spark의 설치보다는 yarn과의 연동부분을 중점으로 둡니다.   Python 설치 (pyspark를 위함) Spark 계정 설정 Spark 다운로드 및 설치 Spa…","fields":{"slug":"/data-sparkonyarn/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Apache Spark v3.0 on yarn 설치 With Zeppelin","date":"August 16, 2021"}},"next":{"fields":{"slug":"/data-hadoopinstall/"}},"previous":{"fields":{"slug":"/azure-aksconnect/"}}},{"node":{"id":"cc3f2da4-d9dd-5454-a99e-eba728ad8ea2","excerpt":"머리말   앞이 막막합니다. 저번 포스트에서 이미 인프라 구성을 끝냈어야 했는데… 이번 포스트에서라도 마무리 지어보죠   ✔ 설치 환경 Hadoop 3.3.0 (Full-Distribute Mode) Server Master Worker1 Worker2 OS CentOS 8.2 CentOS 8.2 CentOS 8.2 Disk 30G 30G 30G MEM 14G 14G 14G CPU 4.Core 4.Core 4.Core VM (Azure) Hadoop Master IP : 10.0.0.5  Hadoop Worker1 IP : 10.0.0.6  Hadoop Worker2 IP : 10.0.0.7  ✌ Hadoop 설치 전…","fields":{"slug":"/data-hadoopinstall/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Hadoop 3.3.0 Full Distribute mode infra 구축","date":"August 15, 2021"}},"next":{"fields":{"slug":"/data-sparkinstall/"}},"previous":{"fields":{"slug":"/data-sparkonyarn/"}}},{"node":{"id":"55f2967c-b271-5121-9dc5-8bebc2e1c7a9","excerpt":"머리말   저번 포스트에서 Apache Spark가 어떤 식으로 동작하는지? 어떤 함수가 있는지? 간단하게 이론적으로만 알아봤습니다. 아직 Spark에 대한 내용이 제대로 이해가 되지 않아 일단 구성부터 해보고 실습을 하면서 다시 이해를 해보겠습니다.   ✔ Azure VM에 Spark StandAlone 구성 Spark StandAlone Cluster로 구성하는 포스트입니다.  환경구성 OS : CentOS Linux release 8.2.2004 (Core)   cpu : 4 core   RAM : 14GB   JDK : 1.8.0 python : 3.8.8 Spark 3.0.2 zeppelin : 0.9.0 1…","fields":{"slug":"/data-sparkinstall/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Azure VM에 Apache Spark v3.0 Standalone 설치 With Zeppelin","date":"August 14, 2021"}},"next":{"fields":{"slug":"/data-hadoopeco/"}},"previous":{"fields":{"slug":"/data-hadoopinstall/"}}},{"node":{"id":"3b573dbf-a917-56a5-9a76-6736d98f7ec3","excerpt":"머리말   이전 포스트에서 Hadoop EcoSystem 중 Core Project에 대해서 다뤘었습니다. 이번 포스트에서는 데이터를 수집하거나 DB화 하는 오픈소스들의 모음인 SUB Project들에 대해서 다룹니다. 모든 프로젝트를 다루지는 않고 앞으로 사용하게 될 것 같은 프로젝트 위주로 정리했습니다.     ✔ Hadoop EcoSystem Sub Project 123123123 이전포스트에서는 Hadoop EcoSystem의 Core Project 부분에 대해서 다뤘습니다. Core Project는 다 설명했고 이제 Hadoop Sub Project의 차례 입니다.  Hadoop Core Project : H…","fields":{"slug":"/data-hadoopeco/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Hadoop EcoSystem Sub Project","date":"August 13, 2021"}},"next":{"fields":{"slug":"/data-hadoop/"}},"previous":{"fields":{"slug":"/data-sparkinstall/"}}},{"node":{"id":"e82d4a67-91b1-5218-9fc2-35c7377e5ffc","excerpt":"머리말   이번 내용은 이전에 Spark의 이론적인 설명을 이어서 더 대표적인 Hadoop에 대해서 이론적인 내용들을 정리해보는 포스트입니다. 저는 여러 포스트로 실제 Cluster를 구축하긴 했지만 HDFS가 데이터를 어떻게 저장하는지, ecosystem이 뭐지? 라는 의문이 많이 남았기에 궁금한 내용들을 정리할 필요를 느꼈습니다.   ✔ Apache Hadoop? 1111123123 Hadoop : 하둡 소프트웨어 라이브러리는 간단한 프로그래밍 모델을 사용하여 여러대의 컴퓨터 클러스터에서 대규모 데이터 세트를 분산 처리 할 수있게 해주는 프레임워크 이다. 라고 모든 글에서 설명을 하는데 나는 그냥 데이터를 분산 저…","fields":{"slug":"/data-hadoop/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Apache Hadoop, HDFS, MapReduce","date":"August 13, 2021"}},"next":{"fields":{"slug":"/date-spark/"}},"previous":{"fields":{"slug":"/data-hadoopeco/"}}},{"node":{"id":"2a5bcac3-907a-58f0-81da-8f92654bc3e6","excerpt":"머리말   이번에는 데이터의 가장 기초적인 오픈소스인 Apache Spark에 대한 내용 정리입니다. 아무것도 모르는 생짜 초보이기 때문에 틀린 부분이 많을 수 있습니다.   ✔ Apache Spark? Hadoop? 캡처1 주워들은 말로는 데이터 시장은 오픈소스인 Hadoop과 Apache가 경쟁하며 성장하고 있다고 알고 있다 그런데 또 다른 글들을 보니 이미 업계에서는 두 오픈소스를 동시에 사용한다고도 한다. 경쟁하는 관계인데 또 상생을 하고 있다는게 무슨소리지? 다시 한번 찾아보니 각각의 툴의 용도에 대해서 알지 못했던 나의 오착이었다.   내가 이해한 두 앱의 용도를 간단하게 설명해보면 우선 두 툴은 빅데이터 …","fields":{"slug":"/date-spark/"},"frontmatter":{"categories":"DATA","title":"[DATA] - Apache Spark란??","date":"August 13, 2021"}},"next":{"fields":{"slug":"/devops-sonarqube/"}},"previous":{"fields":{"slug":"/data-hadoop/"}}},{"node":{"id":"887be352-bf3c-5019-b2eb-f2dcb2cfba4d","excerpt":"머리말   블로그에도 매번 인프라나 Devops 관련 글들만 올라와서 최근에 공부하고 있는 Data쪽도 포스트를 늘리려고 합니다. 아직 초급자 수준이라서 틀린 내용이 많을 것 같지만, 복습하는 느낌으로… 본 포스트에서 내용들은 모드 MS Doc를 기준으로 정리해 작성했습니다. ✔ Azure Synapse Analytics Synapse Analytics는 엔터프라이즈 데이터 웨어하우징과 빅 데이터 분석을 결합한 SaaS 입니다. Synapse의 용어 중의 SQL Pool (SQL DW)이란?? Synapse Analytics에서 사용할 수 있는 을 나타냅니다.  Enter Prise Data WareHousing 엔터…","fields":{"slug":"/azure-synapse/"},"frontmatter":{"categories":"CLOUD DATA","title":"[AZURE] [DATA] Azure Synapse Analytics","date":"August 02, 2021"}},"next":{"fields":{"slug":"/azure-web/"}},"previous":{"fields":{"slug":"/devops-cicd1/"}}}]}},"staticQueryHashes":["1073350324","2938748437"]}