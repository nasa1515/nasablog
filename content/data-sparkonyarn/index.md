---
emoji: ğŸ¤¦â€â™‚ï¸
title:  Apache Spark v3.0 on yarn ì„¤ì¹˜ With Zeppelin [DATA]
date: "2021-08-16 00:39:25"
author: nasa1515
tags: DATA
categories: DATA
---





ë¨¸ë¦¬ë§  

ì´ì „ì— í•œë²ˆ Standalone Clusterë¡œ Sparkë¥¼ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤.  
ê·¸ëŸ¬ë‚˜ Azureì™€ ì—°ë™í•˜ëŠ” ê³¼ì •ì—ì„œ ì—¬ëŸ¬ê°€ì§€ ë¬¸ì œê°€ ë°œìƒí–ˆê³   
ê²°êµ­ ì´ì „ í¬ìŠ¤íŠ¸ì¸ Hadoop Clusterë¥¼ êµ¬ì„±í•´ì„œ Sparkë¥¼ êµ¬ë™ì‹œí‚¤ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.  
ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì„¤ì¹˜í•œ Hadoop Clusterì˜ yarnì— Sparkë¥¼ êµ¬ë™ì‹œí‚¤ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.  






  
--- 

## âœ” Spark ì„¤ì¹˜ 

JDK ë“±ì˜ ê¸°ë³¸ì ì¸ í™˜ê²½ì„¤ì •ì€ [ì´ì „í¬ìŠ¤íŠ¸](https://nasa1515.tech/data-sparkinstall/)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.  
ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Sparkì˜ ì„¤ì¹˜ë³´ë‹¤ëŠ” yarnê³¼ì˜ ì—°ë™ë¶€ë¶„ì„ ì¤‘ì ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.  


<br/>

* #### Python ì„¤ì¹˜ (pysparkë¥¼ ìœ„í•¨)  

    ```cs
    ### ì‚¬ì „ íŒŒì¼ ì„¤ì¹˜
    [root@hadoop-master hadoop]# yum -y install gcc openssl-devel bzip2-devel libffi-devel make


    ### íŒŒì´ì¬ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

    [root@hadoop-master home]# wget https://www.python.org/ftp/python/3.8.8/Python-3.8.8.tgz
    [root@hadoop-master home]# tar xvfz Python-3.8.8.tgz
    [root@hadoop-master home]# rm -rf Python-3.8.8.tgz
    [root@hadoop-master home]# chmod -R 777 Python-3.8.8/
    [root@hadoop-master Python-3.8.8]# ./configure --enable-optimizations
    [root@hadoop-master Python-3.8.8]# make altinstall
    [root@hadoop-master Python-3.8.8]# echo alias python="/usr/local/bin/python3.8" >> /root/.bashrc
    [root@hadoop-master Python-3.8.8]# source /root/.bashrc
    [root@hadoop-master Python-3.8.8]# python -V
    Python 3.8.8
    [root@hadoop-master Python-3.8.8]# which python
    alias python='/usr/local/bin/python3.8'
            /usr/local/bin/python3.8
    ```

<br/>

* #### Spark ê³„ì • ì„¤ì •  

    ```cs
    [root@hadoop-master ~]# useradd spark
    [root@hadoop-master ~]# passwd spark
    [root@hadoop-master ~]# usermod -G wheel spark
    ```

<br/>

* #### Spark ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜  

    ```cs
    ### Spark ê³„ì • ìƒì„± ë° ì„¤ì •

    [root@hadoop-master ~]# useradd spark
    [root@hadoop-master ~]# passwd spark
    [root@hadoop-master ~]# usermod -G wheel spark

    ### Spark 3.0.2 Version ë‹¤ìš´ë¡œë“œ

    [root@hadoop-master spark]# wget https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz
    --2021-03-10 01:27:27--  https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz

    ### ì••ì¶• í•´ì œ ë° ê¶Œí•œ ì„¤ì •
    [root@hadoop-master spark]# tar xvfz spark-3.0.2-bin-hadoop2.7.tgz
    [root@hadoop-master spark]# mv spark-3.0.2-bin-hadoop2.7 spar
    [root@hadoop-master spark]# chown -R spark:spark spark
    [root@hadoop-master spark]# chmod -R 777 spark
    ```

<br/>

* #### Spark í™˜ê²½ë³€ìˆ˜ ë“±ë¡ (Spark ê³„ì •, Hadoop ê³„ì • ì§„í–‰)  


    ```cs
    [spark@hadoop-master ~]$ echo export PATH='$PATH':/home/spark/spark/bin >> ~/.bashrc
    [spark@hadoop-master ~]$ echo export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64" >> ~/.bashrc
    [spark@hadoop-master ~]$ tail -1 ~/.bashrc
    export PATH=$PATH:/home/spark/spark/bin
    [spark@hadoop-master ~]$ soruce ~/.bashrc
    bash: soruce: command not found
    [spark@hadoop-master ~]$ source ~/.bashrc
    [spark@hadoop-master ~]$ echo $PATH
    /home/spark/.local/bin:/home/spark/bin:/home/spark/.local/bin:/home/spark/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin::/root/bin:/home/spark/spark/bin
    ```

    <br/>

* #### Spark Config ì„¤ì • 

    ```cs
    #### spark-env.sh ì— ë‹¤ìŒ ì„¤ì • ì¶”ê°€

    ## nasa settin

    ## nasa settin

    export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64"
    export SPARK_WORKER_INSTANCES=2
    export PYSPARK_PYTHON="/usr/bin/python3"
    export HADOOP_HOME="/usr/local/hadoop"
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop    


    #### spark-defaults.conf ì„¤ì •

    [spark@hadoop-master conf]$ cp spark-defaults.conf.template spark-defaults.conf
    [spark@hadoop-master conf]$ vim spark-defaults.conf

    ## ì„¤ì • ì¶”ê°€    
    spark.master                    yarn
    spark.deploy.mode               client
    ```

<br/>


* #### Spark, pyspark ë™ì‘ í™•ì¸ 

    ```cs
    ### Spark-shell ë™ì‘í™•ì¸

    [spark@hadoop-master root]$ spark-shell
    21/03/10 01:56:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    Setting default log level to "WARN".
    To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    Spark context Web UI available at http://hadoop-master:4040
    Spark context available as 'sc' (master = local[*], app id = local-1615341369333).
    Spark session available as 'spark'.
    Welcome to
        ____              __
        / __/__  ___ _____/ /__
        _\ \/ _ \/ _ `/ __/  '_/
    /___/ .__/\_,_/_/ /_/\_\   version 3.0.2
        /_/

    Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_275)
    Type in expressions to have them evaluated.
    Type :help for more information.

    scala>




    ### pyspark ë™ì‘ í™•ì¸

    [spark@hadoop-master conf]$ pyspark
    Python 3.6.8 (default, Apr 16 2020, 01:36:27)
    [GCC 8.3.1 20191121 (Red Hat 8.3.1-5)] on linux
    Type "help", "copyright", "credits" or "license" for more information.
    21/03/10 01:59:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    Setting default log level to "WARN".
    To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    Welcome to
        ____              __
        / __/__  ___ _____/ /__
        _\ \/ _ \/ _ `/ __/  '_/
    /__ / .__/\_,_/_/ /_/\_\   version 3.0.2
        /_/

    Using Python version 3.6.8 (default, Apr 16 2020 01:36:27)
    SparkSession available as 'spark'.
    >>> 1+2
    3
    ```

<br/>

* #### YARN ì—ì„œ SPARK APPì´ ì œëŒ€ë¡œ êµ¬ë™ë˜ëŠ”ì§€ í™•ì¸ ê²¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë§Œë“¤ì–´ë³´ì£  


    ì–´ì œ hdfsì— ì˜¬ë ¤ë…¼ test dataì˜ sessionìˆ˜ë¥¼ ì–»ëŠ” ê°„ë‹¨í•œ ìŠ¤í¬ë¦½íŠ¸ 

    ```cs
    [hadoop@hadoop-master nasa1515]$ hdfs dfs -ls /
    Found 1 items
    -rw-r--r--   3 hadoop supergroup  500253789 2021-03-09 08:47 /nasa.csv
    ```

    ```cs
    from pyspark.sql import SparkSession

    spark = SparkSession \
        .builder \
        .appName("Python Spark SQL basic example") \
        .getOrCreate()

    df = spark.read.option("header","true").csv('hdfs:/nasa.csv').cache()

    df.show()
    ```

<br/>

* #### í•´ë‹¹ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ëŒë ¤ë´…ì‹œë‹¤ 

    ```cs
    [spark@hadoop-master conf]$ spark-submit --master yarn --deploy-mode client --executor-memory 1g /home/spark/test.py
    2021-03-10 02:20:10,493 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    2021-03-10 02:20:11,137 INFO spark.SparkContext: Running Spark version 3.0.2
    2021-03-10 02:20:11,177 INFO resource.ResourceUtils: ==============================================================
    2021-03-10 02:20:11,185 INFO resource.ResourceUtils: Resources for spark.driver:
    ...
    ...(ì¤‘ëµ)

    2021-03-10 02:20:40,402 INFO spark.SparkContext: Invoking stop() from shutdown hook
    2021-03-10 02:20:40,410 INFO server.AbstractConnector: Stopped Spark@6f05fe89{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
    2021-03-10 02:20:40,412 INFO ui.SparkUI: Stopped Spark web UI at http://hadoop-master:4040
    2021-03-10 02:20:40,416 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
    2021-03-10 02:20:40,438 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
    2021-03-10 02:20:40,438 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
    2021-03-10 02:20:40,443 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped
    2021-03-10 02:20:40,452 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
    2021-03-10 02:20:40,463 INFO memory.MemoryStore: MemoryStore cleared
    2021-03-10 02:20:40,464 INFO storage.BlockManager: BlockManager stopped
    2021-03-10 02:20:40,469 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
    2021-03-10 02:20:40,475 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
    2021-03-10 02:20:40,497 INFO spark.SparkContext: Successfully stopped SparkContext
    2021-03-10 02:20:40,498 INFO util.ShutdownHookManager: Shutdown hook called
    2021-03-10 02:20:40,498 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b04d174a-0be5-44ee-87ad-8915e64b3d51
    2021-03-10 02:20:40,501 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b04d174a-0be5-44ee-87ad-8915e64b3d51/pyspark-5cf80ddf-89c1-4330-93c8-28e2f93b6c08
    2021-03-10 02:20:40,510 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-dd89ea67-71be-4e21-ba16-ee7623fea72a
    ```

<br/>

* #### ì´ì œ Hadoopì˜ Manager WEBì—ì„œ í™•ì¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.  

    ![123123123123](https://user-images.githubusercontent.com/69498804/110577703-858b8680-81a6-11eb-879b-2ad3f739c549.JPG)


<br/>

* #### ê·¸ëŸ¼ sparkë¥¼ ê°€ë™ì‹œì¼œì£¼ê³ 

    ```cs
    [spark@hadoop-master sbin]$ start-master.sh
    starting org.apache.spark.deploy.master.Master, logging to /home/spark/spark/logs/spark-spark-org.apache.spark.deploy.master.Master-1-hadoop-master.out

    [spark@hadoop-master sbin]$ start-slave.sh spark://hadoop-master:7077
    starting org.apache.spark.deploy.worker.Worker, logging to /home/spark/spark/logs/spark-spark-org.apache.spark.deploy.worker.Worker-1-hadoop-master.out
    starting org.apache.spark.deploy.worker.Worker, logging to /home/spark/spark/logs/spark-spark-org.apache.spark.deploy.worker.Worker-2-hadoop-master.out
    ```

<br/>

---

## âœŒ Zeppelin ì„¤ì¹˜ í›„ ì—°ë™ì„ í™•ì¸í•´ë´…ì‹œë‹¤.

* #### Zepplein ì„¤ì¹˜ ë° ê¶Œí•œ ì„¤ì •

    ```cs
    [root@hadoop-master ~]# useradd zeppelin
    [root@hadoop-master ~]# passwd zeppelin
    [root@hadoop-master ~]# cd /home/zeppelin/
    [root@hadoop-master zeppelin]# wget https://downloads.apachewget https://downloads.apache.org/zeppelin/zeppelin-0.9.0-preview2/zeppelin-0.9.0-preview2-bin-all.tgz
    [root@hadoop-master zeppelin]# tar xvfz zeppelin-0.9.0-preview2-bin-all.tgz
    [root@hadoop-master zeppelin]# mv zeppelin-0.9.0-preview2-bin-all zeppelin
    [root@hadoop-master zeppelin]# chown -R zeppelin:zeppelin zeppelin
    [root@hadoop-master zeppelin]# chmod -R 777 zeppelin
    ```

    <br/>

* #### í™˜ê²½ë³€ìˆ˜ ì„¤ì • [zeppelin ê³„ì •]


    ```cs
    [zeppelin@hadoop-master ~]$ echo export PATH="$PATH:/home/zeppelin/zeppelin/bin" >> ~/.bashrc
    [zeppelin@hadoop-master ~]$ source ~/.bashrc

    ### ì´ ë‚´ìš© 

    export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64"
    export HADOOP_HOME="/usr/local/hadoop"
    export SPARK_HOME="/home/spark/spark"
    export PATH="$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:"
    export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
    export PATH=$PATH:$SPARK_HOME/bin:$HADDOP_HOME/bin:$HADOOP_HOME/sbin
    ```

    <br/>

* #### Zeppelin í™˜ê²½ ì„¤ì • 

    ```cs
    [zeppelin@hadoop-master ~]$ cd /home/zeppelin/zeppelin/conf
    [zeppelin@hadoop-master conf]$ cp zeppelin-env.sh.template zeppelin-env.sh


    ### ì„¤ì • ì¶”ê°€


   
    export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64"
    export SPARK_HOME="/home/spark/spark"
    export MASTER=yarn-client
    export HADOOP_HOME="/usr/local/hadoop"
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop


    ### zeppelin-site.xml ìˆ˜ì •

    zeppelin-site.xml
    ...
    ...
    <property>
    <name>zeppelin.server.addr</name>
    <value>10.0.0.5</value> -> Client IPë¡œ ë³€ê²½
    <description>Server binding address</description>
    </property>

    <property>
    <name>zeppelin.server.port</name>
    <value>7777</value>  -> 8080ì€ Sparkê°€ ì“°ê³ ìˆê¸°ì— 7777ë¡œ ì„¤ì •
    <description>Server port.</description>
    </property>
    ...
    ...
    ```

    <br/>


* #### Zeppelin ê¸°ë™ í…ŒìŠ¤íŠ¸ 

    ```cs
    [zeppelin@hadoop-master conf]$ zeppelin-daemon.sh start
    ```


    ![12313231](https://user-images.githubusercontent.com/69498804/110597015-651df500-81c3-11eb-8cb1-371a512f7b8b.JPG)

    <br/>


* #### ì•„ì°¸! zeppelin-env ì„¤ì •ì´ ì ìš©ì´ ì•ˆë˜ëŠ” ì´ìŠˆê°€ ìˆì—ˆìŠµë‹ˆë‹¤ ã… ã…  
    ê·¸ë˜ì„œ ì €ëŠ” ì§ì ‘ zeppelin webì˜ Interpreter ì„¤ì •ì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.  
    ![ìº¡ì²˜1231321](https://user-images.githubusercontent.com/69498804/110597241-9e566500-81c3-11eb-980d-43b03fa5d704.JPG)

    <br/>

    ë‹¤ìŒê³¼ ê°™ì´ Spark.matser, deploymodeë¥¼ ìˆ˜ì •í•´ì¤ë‹ˆë‹¤. 

    ![12312313231](https://user-images.githubusercontent.com/69498804/110597343-c645c880-81c3-11eb-9c6c-06b1ce953c44.JPG)

<br/>


* #### ê·¸ëŸ¼ ì•„ê¹Œ testë¡œ ì‘ì„±í–ˆë˜ ì½”ë“œê°€ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì£  

    ![12312312312312312321312](https://user-images.githubusercontent.com/69498804/110597530-060cb000-81c4-11eb-933f-6b914cabd5b4.JPG)

    ì•„ì£¼ ì˜ ëŒì•„ê°‘ë‹ˆë‹¤. 


<br/>


* #### ê·¸ëŸ¼ yarn managerì—ì„œë„ í™•ì¸ì´ ê°€ëŠ¥í•œì§€ ë´…ì‹œë‹¤

    ![1111111](https://user-images.githubusercontent.com/69498804/110597695-3c4a2f80-81c4-11eb-9c1a-365dc2c8d727.JPG)

    ë‹¤ìŒê³¼ ê°™ì´ zeppelin appì˜ ë™ì‘ì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!! 

<br/>

---

## ë§ˆì¹˜ë©°â€¦  

  
ì—¬ëŸ¬ê°€ì§€ ë¬¸ì œê°€ ìˆì—ˆì§€ë§Œ ê·¸ë˜ë„ í•˜ë£¨ë§Œì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤.  
ì´ì œ ë“œë””ì–´ pyspark ë¬¸ë²•ì´ë‚˜ dataë¥¼ ë‹¤ë£¨ëŠ” ë°©ë²•ë“¤ì— ëŒ€í•´ì„œ ì‹¤ìŠµí•˜ê² ë„¤ìš”.  
ì¶”ê°€ì ìœ¼ë¡œ ambari ì„¤ì¹˜í•´ì„œ í´ëŸ¬ìŠ¤í„°ë„ ëª¨ë‹ˆí„°ë§ í•´ë³´ê² ìŠµë‹ˆë‹¤.  


<br/>

---


```toc
```