---
emoji: ğŸ¤¦â€â™‚ï¸
title: "[DATA] - GCP DataFlow, csv from GCS to BigQuery With Python"
date: "2021-09-02 00:39:25"
author: nasa1515
tags: DATA GCP
categories: DATA GCP
---



ë¨¸ë¦¬ë§  

ìš”ì¦˜ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„± í•  ì‹œê°„ì´ ë¶€ì¡±í–ˆìŠµë‹ˆë‹¤...(ì¼...) ê·¸ë˜ì„œ ì˜¤ëœë§Œì— í¬ìŠ¤íŠ¸ë¥¼ ì˜¬ë¦° ê¸°ë…ìœ¼ë¡œ ì´ë²ˆ ë‚´ìš©ì„ ë”ìš± ì•Œì°¨ê²Œ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.  
ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” GCPì˜ DataFlowë¥¼ ì‚¬ìš©í•´ GCSì— ìˆëŠ” CSV íŒŒì¼ì„ ê°„ë‹¨í•œ Parsing ì‘ì—…ì„ í•œ ë’¤   
BigQuery Tableì— ì ì¬í•˜ëŠ” ë¶€ë¶„ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. ë¬¼ë¡  íŒŒì´ì¬ì„ ì²¨ê°€í•´ì„œ  

<br/>

---


## âœ” DataFlowì— ëŒ€í•´ì„œ..


DataFlowëŠ” GCPì—ì„œ DataPipeline(ETL, MR ë“±)ì„ Apache Beam ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ë„ë¡ ë§Œë“  Runtime Service ì…ë‹ˆë‹¤.  
ìŒ ê°„ë‹¨í•˜ê²Œ ë§í•˜ë©´ Spark Stremingì´ë‚˜ Batch ì²˜ë¦¬ë¥¼ Cloudë¥¼ ì‚¬ìš©í•´ PaaSë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.  
ë‹¨ Apache beamì— ì¢…ì†ë˜ì–´ ìˆì–´ì„œ beam SDKë¥¼ ë´ì•¼í•˜ëŠ” ë¶ˆí¸í•œ ë¶€ë¶„ì€ ìˆìŠµë‹ˆë‹¤..   

<br/>

* ì¼ë‹¨ ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ê°„ë‹¨í•˜ê²Œ êµ¬ì„±í•˜ë ¤ê³  í•˜ëŠ” ì•„í‚¤í…ì³ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

    ![ìº¡ì²˜](https://user-images.githubusercontent.com/69498804/116188577-9bf39e80-a762-11eb-8d0a-ad2989bbaa14.JPG)

    * Batch í˜•íƒœì˜ Data [CSV]ë¥¼ GCSì— Uploadí•˜ë©´ í•´ë‹¹ File Parsing í›„ DWì— ì ì¬. 

    <br/>


<br/>

---

## âœŒ DataFlow ì‚¬ìš©ì„ ìœ„í•œ í™˜ê²½ êµ¬ì„±

ì €ëŠ” Localì˜ VScodeì—ì„œ Codeë¥¼ ì‚¬ìš© í•  ê²ƒì´ê¸° ë•Œë¬¸ì—   
DataFlow ì‚¬ìš©í•˜ê¸° ìœ„í•œ í™˜ê²½ì„ êµ¬ì„±í•˜ëŠ” ê²ƒë¶€í„° ì§„í–‰í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. 

<br/>

* #### DataFlowë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ APIë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤. [ì¶”ê°€í•´ì¤ë‹ˆë‹¤.]  

    ![ìº¡ì²˜2](https://user-images.githubusercontent.com/69498804/113811074-806e2700-97a6-11eb-9e4b-384907be0558.JPG)


    ì„¤ì¹˜ API ëª©ë¡

    * Cloud Dataflow
    * Stackdriver
    * Cloud Storage
    * Cloud Storage JSON
    * BigQuery
    * Cloud Pub/Sub
    * Cloud Datastore
    * Cloud Resource Manager APIs


    <br/>
<br/>

* #### ì´í›„ ìƒˆë¡œìš´ Service Accountë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ![ìº¡ì²˜3](https://user-images.githubusercontent.com/69498804/113811448-30439480-97a7-11eb-9a42-4e8425375130.JPG)

    * ê¶Œí•œ : ì†Œìœ ì  
    * KeyFile : Json  
        * KeyFilwì„ Localë¡œ ë°›ì•„ë†”ì•¼ í•©ë‹ˆë‹¤!

    <br/> 
<br/>

* #### ì´ì œ GCS(Google Cloud Storage)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 

    ![ìº¡ì²˜4](https://user-images.githubusercontent.com/69498804/113811803-d68f9a00-97a7-11eb-8cc0-0d6463f8a42b.JPG)


    * Storage Class : Standard
    * Single Region 
        * ê°€ê¸‰ì ì´ë©´ ë¹„ìš©ì ìœ¼ë¡œë¼ë„ BigQueryë¥¼ ì‚¬ìš© í•  Regionê³¼ ë§ì¶°ì£¼ì„¸ìš”!


    <br/>
<br/>

* #### ìµœì¢…ì ìœ¼ë¡œ Dataë¥¼ ì ì¬ í•  Bigquery DataSetì„ ìƒì„±í•©ë‹ˆë‹¤.  

    ![ìº¡ì²˜2](https://user-images.githubusercontent.com/69498804/116189103-6dc28e80-a763-11eb-88f6-1a8e49ea32b8.JPG)

    * BigQuery Project : lws-cloocus
    * BigQuery DataSet : Test 



ì ì—¬ê¸°ê¹Œì§€ ê°„ë‹¨í•œ í™˜ê²½ì„¤ì •ì€ ì™„ë£Œë˜ì—ˆê³  Codeë¥¼ ì„¤ëª…í•˜ë©´ì„œ ì¶”ê°€ì ìœ¼ë¡œ ë³´ê² ìŠµë‹ˆë‹¤.  

<br/>

---

## ğŸ‘ python code <a name="a1"></a> 


* ìµœì¢…ì ì¸ DataPipelineì˜ Python Code ì…ë‹ˆë‹¤.

    ```python
    from __future__ import absolute_import
    import argparse
    import logging
    import re
    import apache_beam as beam
    import json
    import os

    from apache_beam.io import ReadFromText
    from apache_beam.io import WriteToText
    from apache_beam.metrics import Metrics
    from apache_beam.options.pipeline_options import PipelineOptions
    from apache_beam.options.pipeline_options import SetupOptions
    from apache_beam.options.pipeline_options import StandardOptions
    from apache_beam.options.pipeline_options import GoogleCloudOptions
    from apache_beam.pipeline import PipelineOptions
    from google.cloud import storage
    from google.oauth2 import service_account
    import pandas as pd


    # GCP Service Account Key env ìœˆë„ìš°ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ê°€ëŠ¥
    storage_client = storage.Client.                            from_service_account_json                                                   ('C:\GCP\lws-cloocus-d4fde98375c7.json')
        

    # for linux "service account key" 

    #GOOGLE_APPLICATION_CREDENTIALS('/home/nasa1515/dataflow/lwskey.json')
    
    # word length code

    class WordExtractingDoFn(beam.DoFn):
    def process(self, element):
        
        splited = element.split(',')
        writestring = ({'id': splited[0], 'price': splited[1], 'manufacturer': splited[2], 'condition': splited[3]})
        #writestring = {'splited[0], splited[1], splited[2], splited[3]'}
        return [writestring]

    # parser option code
    def run(argv=None, save_main_session=True):
 
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input',dest='input',required=False,help='default'
        ,default='gs://storage_nasa1515/data/batch2.csv')
    parser.add_argument(
        '--output',dest='output',required=False,help='default'
        ,default='lws-cloocus:nasa1515.batchtest')
 
    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_options = PipelineOptions(pipeline_args)

    # pipline option

    google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)
    google_cloud_options.project = 'lws-cloocus'
    google_cloud_options.job_name = 'test-to-big'
    google_cloud_options.staging_location = 'gs://storage_nasa1515/staging'
    google_cloud_options.temp_location = 'gs://storage_nasa1515/temp'
    google_cloud_options.region = 'asia-northeast3'
    pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'
    
  
    # # test1

    p = beam.Pipeline(options = PipelineOptions(pipeline_args))

    with beam.Pipeline(options=pipeline_options) as p:
 
        table_schema = {
            'fields': [
                {"name": "id", "type": "STRING", "mode": "NULLABLE"}, 
                {"name": "price", "type": "INTEGER", "mode": "NULLABLE"},
                {"name": "manufacturer", "type": "STRING", "mode": "NULLABLE"},
                {"name": "condition", "type": "STRING", "mode": "NULLABLE"}
            ]
        }
        
        (p 
            | 'Read Data' >> ReadFromText(known_args.input)

            | beam.ParDo(WordExtractingDoFn(WordExtractingDoFn))
            | 'write to BigQuery' >> beam.io.WriteToBigQuery(
                known_args.output,
                schema = table_schema,
                method=beam.io.WriteToBigQuery.Method.FILE_LOADS,
                create_disposition = beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition = beam.io.BigQueryDisposition.WRITE_TRUNCATE
            )
        )

    result = p.run()
    result.wait_until_finish()

    if __name__ == '__main__':
        logging.getLogger().setLevel(logging.INFO)
        run()
    ```

<br/>
<br/>

* ê¸°ë³¸ì ìœ¼ë¡œ GCP DataFlowì—ì„œ beam SDKë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  ì•„ë˜ SDKì˜ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. 

    ```cs
    pip install apache-beam[gcp]
    ```

<br/>
<br/>


* SDK ì„ ì–¸ ì½”ë“œ 

    ```python
    from __future__ import absolute_import
    import argparse
    import logging
    import re
    import apache_beam as beam
    import json
    import os
    import pandas as pd

    from apache_beam.io import ReadFromText
    from apache_beam.io import WriteToText
    from apache_beam.metrics import Metrics
    from apache_beam.options.pipeline_options import PipelineOptions
    from apache_beam.options.pipeline_options import SetupOptions
    from apache_beam.options.pipeline_options import StandardOptions
    from apache_beam.options.pipeline_options import GoogleCloudOptions
    from apache_beam.pipeline import PipelineOptions
    from google.cloud import storage
    from google.oauth2 import service_account
    ```
    
<br/>
<br/>

* ê¶Œí•œì„ ì–»ê¸° ìœ„í•œ ì½”ë“œ 

    ì•„ê¹Œ Service Accountë¥¼ ìƒì„±í•˜ë©° ë°œê¸‰ ë°›ì•˜ë˜ Keyì˜ ìœ„ì¹˜ë¥¼ ì„ ì–¸í•´ì¤ë‹ˆë‹¤.  
    Window/Linux ëª¨ë‘ í™˜ê²½ë³€ìˆ˜ë¡œ GOOGLE_APPLICATION_CREDENTIALS ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.

    ```python
    # GCP Service Account Key env
    storage_client = storage.Client.from_service_account_json('C:\GCP\lws-cloocus-d4fde98375c7.json')
        

    # for linux/window "service account key" 

    #GOOGLE_APPLICATION_CREDENTIALS('/home/nasa1515/dataflow/lwskey.json')
    ```

<br/>
<br/>

* íŒŒì´í”„ë¼ì¸ ì„¤ì • ì½”ë“œ

    ```python
    ## ,ìœ¼ë¡œ êµ¬ë¶„ëœ CSV Fileì„ Bigqueryê°€ ì¸ì‹í•˜ëŠ” ,ìœ¼ë¡œ êµ¬ë¶„ëœ Json Fileë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œ

    class WordExtractingDoFn(beam.DoFn):
    def process(self, element):
        
        splited = element.split(',')
        writestring = ({'id': splited[0], 'price': splited[1], 'manufacturer': splited[2], 'condition': splited[3]})
        #writestring = {'splited[0], splited[1], splited[2], splited[3]'}
        return [writestring]


    ## pasing í•  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  write í•  Bigqueryì— ëŒ€í•œ ì •ë³´ë“¤ì„ ì…ë ¥

    # parser option code
    def run(argv=None, save_main_session=True):
 
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input',dest='input',required=False,help='default'
        ,default='gs://storage_nasa1515/data/batch2.csv')
    parser.add_argument(
        '--output',dest='output',required=False,help='default'
        ,default='lws-cloocus:nasa1515.batchtest')
 
    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_options = PipelineOptions(pipeline_args)

    # pipline option

    google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)
    google_cloud_options.project = 'lws-cloocus'
    google_cloud_options.job_name = 'test-to-big'
    google_cloud_options.staging_location = 'gs://storage_nasa1515/staging'
    google_cloud_options.temp_location = 'gs://storage_nasa1515/temp'
    google_cloud_options.region = 'asia-northeast3'
    pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'
    ```

    * ì´ë¶€ë¶„ì—ì„œ ì£¼ìš”í•˜ê²Œ ë´ì•¼ í•  ì ì„ Bigqueryê°€ ë°›ì•„ ë“¤ì¼ ìˆ˜ ìˆëŠ” íŒŒì¼ì˜ í˜•ì‹ì…ë‹ˆë‹¤.  

<br/>
<br/>

* ì œê°€ Pasing í•˜ë ¤ê³  í•˜ëŠ” Batch ì„± ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    ![ìº¡ì²˜](https://user-images.githubusercontent.com/69498804/116191285-06a6d900-a767-11eb-8860-ad389d2b7e0f.JPG)

<br/>
<br/>

* ê·¸ëŸ¬ë‚˜ Bigqueryì—ì„œëŠ” ë¯¸ë¦¬ ì •ì˜í•œ Schema í˜•íƒœì˜ ,JSONìœ¼ë¡œë§Œ Dataë¥¼ Load í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

    ![ìº¡ì²˜2](https://user-images.githubusercontent.com/69498804/116191459-4cfc3800-a767-11eb-9b76-9717d755dfca.JPG)

<br/>
<br/>

* ë§Œì•½ í•´ë‹¹ Data í˜•íƒœë¡œ Parsingì´ ë˜ì§€ ì•Šìœ¼ë©´ ì•„ë˜ì™€ ê°™ì€ Errorê°€ ë°œìƒí•˜ë©° Jobì´ ë©ˆì¶¥ë‹ˆë‹¤.. 

    ![MicrosoftTeams-image](https://user-images.githubusercontent.com/69498804/116191628-9ea4c280-a767-11eb-8c0c-f78ab7588ad3.png)

    * ë•Œë¬¸ì— ìœ„ì˜ ìë£Œí˜•ëŒ€ë¡œ í˜•ì‹ì„ ë§ì¶°ì£¼ëŠ” ê±´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤..[ì—¬ê¸°ì„œ ë»˜ì§“ì„ ë„ˆë¬´ ë§ì´í–ˆì–´ìš”..]


<br/>
<br/>

* ë˜í•œ google_cloud_optionsì— GCSì˜ ê²½ë¡œë“¤ì€ ì „ë¶€ ë¯¸ë¦¬ ìƒì„±ë˜ì–´ìˆì–´ì•¼ í•©ë‹ˆë‹¤. 

    ![ìº¡ì²˜3](https://user-images.githubusercontent.com/69498804/116192136-6487f080-a768-11eb-9054-337c87a348be.JPG)

    * staging
    * temp 

<br/>
<br/>

---


## ğŸ‘€ ì´ì œ ì½”ë“œì— ëŒ€í•œ ì„¤ëª…ì„ ì´ì–´ì„œ í•˜ê² ìŠµë‹ˆë‹¤.


* íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì½”ë“œ 

    ```python
    p = beam.Pipeline(options = PipelineOptions(pipeline_args))

    with beam.Pipeline(options=pipeline_options) as p:
 
        table_schema = {
            'fields': [
                {"name": "id", "type": "STRING", "mode": "NULLABLE"}, 
                {"name": "price", "type": "INTEGER", "mode": "NULLABLE"},
                {"name": "manufacturer", "type": "STRING", "mode": "NULLABLE"},
                {"name": "condition", "type": "STRING", "mode": "NULLABLE"}
            ]
        }
        
        (p 
            | 'Read Data' >> ReadFromText(known_args.input)

            | beam.ParDo(WordExtractingDoFn(WordExtractingDoFn))
            | 'write to BigQuery' >> beam.io.WriteToBigQuery(
                known_args.output,
                schema = table_schema,
                method=beam.io.WriteToBigQuery.Method.FILE_LOADS,
                create_disposition = beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition = beam.io.BigQueryDisposition.WRITE_TRUNCATE
            )
        )

    result = p.run()
    result.wait_until_finish()

    if __name__ == '__main__':
        logging.getLogger().setLevel(logging.INFO)
        run()

    ```

    * ì €ëŠ” ìœ„ì²˜ëŸ¼ table_schemaë¡œ JSON í˜•íƒœì˜ Schemaë¥¼ ë¯¸ë¦¬ ì •ì˜í•´ì„œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.  
    * create_disposition = beam.io.BigQueryDisposition.CREATE_IF_NEEDED
        * í•´ë‹¹ êµ¬ë¬¸ì€ BigQuery DataSetì— Tableì´ ì—†ìœ¼ë©´ ë§Œë“¤ì–´ ì£¼ëŠ” ì˜µì…˜ì…ë‹ˆë‹¤.  


<br/>
<br/>

---

## ğŸ™Œ ì‹¤í–‰ ê²°ê³¼

ì½”ë“œê°€ ëª¨ë‘ ì§œì—¬ì¡Œìœ¼ë‹ˆ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œë³´ì£ .  


<br/>

* ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìœ¼ DataFlow Tabì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ![ìº¡ì²˜4](https://user-images.githubusercontent.com/69498804/116193716-bcbff200-a76a-11eb-887a-6bf57009c06c.JPG)

    * í•­ëª©ë§ˆë‹¤ ì–´ë–¤ ë¶€ë¶„ì„ ì„±ê³µí–ˆëŠ”ì§€ ìì„¸í•˜ê²Œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

<br/>

* DataFlow Jobì´ ì„±ê³µì´ë‹ˆ BigQuery Tableì„ í™•ì¸ í•´ ë³¼ê¹Œìš”?  

    ![ìº¡ì²˜5](https://user-images.githubusercontent.com/69498804/116193951-20e2b600-a76b-11eb-8827-666ff168be00.JPG)

    * ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨í•œ ì¿¼ë¦¬ë¬¸ì„ BigQueryì— ë‚ ë ¤ë³´ë‹ˆ Dataê°€ ì œëŒ€ë¡œ ë“¤ì–´ê°”ë„¤ìš”!  


<br/>

---

## ë§ˆì¹˜ë©°â€¦  

  
í•œ 2~3ì¼ì •ë„ ì‚½ì§ˆì„ ê²½í—˜í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤.  
ì´ë²ˆì—ëŠ” Batch í˜•íƒœì˜ ë°ì´í„°ë§Œ DataFlowë¥¼ ì´ìš©í–ˆì§€ë§Œ ì‚¬ì‹¤ Stremingì˜ ì—­í• ì´ ë” ì¤‘ìš”í•©ë‹ˆë‹¤.  
ê·¸ë˜ì„œ ì¶”í›„ì— Pub/Subê³¼ ì—°ë™í•˜ì—¬ Streming Dataë¥¼ ì ì¬í•˜ëŠ” ë¶€ë¶„ë„ í¬ìŠ¤íŠ¸ í•˜ê² ìŠµë‹ˆë‹¤.   
 
---

```toc
```